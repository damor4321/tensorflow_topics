{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n",
      "97\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))\n",
    "\n",
    "#DAF understanding\n",
    "print(first_letter)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "\n",
    "\n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float) #batch.shape=[64,27], w/ 1-hot-enconding\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      \n",
    "      #DAF understanding\n",
    "      #print(\"letra:\", self._text[self._cursor[b]]) #NO son las letras de una palabra porque cada vez se lee \n",
    "      #la letra de la posicion de un cursor diferente. Hay 64 cursores distribuidos por todo el training_text\n",
    "\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "\n",
    "      #DAF understanding\n",
    "      #print(\"cursor:\", self._cursor) #en cada iteracion (hay 64), se lee 1 de los 64 cursores y se avanza 1 posicion\n",
    "      #el resultado es que con cada batch avanzan 1 posicion los 64 cursores\n",
    "    \n",
    "    return batch\n",
    "\n",
    "\n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    \n",
    "    #DAF understanding\n",
    "    #print(\"->len de batches:\", len(batches), \" shape de cada batch:\",batches[0].shape) \n",
    "    #con num_unrollings=10 y batch_size=64, **batches es una lista de 11 batch, siendo shape de cada batch [64, 27]\n",
    "    #esto es 64 letras leidas del training_text, como se explica en self._next_batch()\n",
    "    return batches\n",
    "\n",
    "\n",
    "def characters(probabilities): #Lo que se pasa es un batch: con 1hot-enc.de 64 letras (shape=[64,27]). Y devuelve las letras\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0] # 64, s es una lista de 64 ''\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))] #se le van uniendo las 64 letras de cada batch, para 11 batch.\n",
    "    #vuelven a aparecer palabras... se vislumbran en los grupos de 11 caracteres\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings) #BatchGenerator(train_text, 64, 10)\n",
    "#DAF num_unrollings(>1) va a implicar un bucle de alimentacion de lstm_cell en el que se iran actualizando \n",
    "#el (saved_)state y el (saved_)output\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1) #BatchGenerator(train_text, 1, 1)\n",
    "#DAF num_unrollings=1 una sola llamada a lstm_cell, y una sola actualizacion de state y output\n",
    "\n",
    "#DAF understanding\n",
    "#b = train_batches._next_batch()\n",
    "#print(b[0])\n",
    "#print(len(b))\n",
    "#print(np.argmax(b, 1))\n",
    "#print(characters(b))\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "#DAF: el primer batch de batches es the last batch of the previous array (last_batch) y aporta al training la primera\n",
    "#letra de cada secuencia. (Hay 1+10 batch en train_batches y 1+1 batch en valid_batches). \n",
    "#Luego le siguen tantos batch como num_unrollings (10 o 1).\n",
    "#El segundo batch de batches es el primero nuevo y aporta la 2ª letra de cada sequencia.\n",
    "#El tercer batch de batches sería el segundo nuevo y aportaría la 3ª letras de la cada secuencuencia, y asi sucesivamente\n",
    "#En train_batches hay 1+10=11 batches de 64 letras (batch_size=64). Y las secuencias son 64 de 11 letras.\n",
    "#En valid_batches hay 1+1=2 batches de 1 letra (batch_size=1). Y las secuencias son 1 secuencia de 2 letras.\n",
    "\n",
    "#DAF: CLAVE HASTA ESTE BLOQUE TODO SE REFIERE AL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00425744  0.03515536  0.0397132   0.0556141   0.03335265  0.04843986\n",
      "   0.06544393  0.05065233  0.00753621  0.0423046   0.00277245  0.02627086\n",
      "   0.04584543  0.05041018  0.01374804  0.03044455  0.05393776  0.06251908\n",
      "   0.01293425  0.03847312  0.05743614  0.05472556  0.0668559   0.0413122\n",
      "   0.00743415  0.03546691  0.01694374]]\n",
      "1.0\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#DAF: CLAVE: TODAS ESTAS FUNCIONES SE USAN EN LA SESSION PARA DAR UN SUMARIO DEL TRAINING, CADA 100 o 1000 STEPS\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0] #labels.shape=(640,27)\n",
    "\n",
    "#DAF: Sólo se usa en la Session. Para calcular las Perplexities de TRAINING y de VALIDATION, CADA 100 steps \n",
    "#En general Perplexiti=np.exp(logprob(predictions, labels) \n",
    "#\n",
    "#En el TRAINING, la \"Perplexity del Minibatch\" se calcula SÓLO sobre el training minibatch actual. \n",
    "#El training minibatch tiene dimensiones 11 x shape[64,27],\n",
    "#De él se extraen las predictions (en Graph, prediction=tf.nn.softmax(logits)): de los 10 primeros batches. Su shape=[640,27]  \n",
    "#así como las labels: de los batch 2º al 11º. La shape de las labels es, igualmente [640,27]\n",
    "#\n",
    "#En la VALIDATION, la Perplexity se calcula sobre TODOS los batches de VALIDATION_TEXT: son 1000 batches\n",
    "#con dimensiones 1+1=2 x shape[1,27]. Se hace un blucle de 1 a 1000 y se calcula la logprob del batch actual.\n",
    "#Esas logprob se van acumulando en una LOGPROB TOTAL, que despues del bucle se usa para calcula la Perplexity\n",
    "#La prediction se extrae evaluar batch[0]. El ultimo del batch anterior. Su shape [1,27]\n",
    "#La label es batch[1]. Su shape [1,27]\n",
    "#\n",
    "#Siempre, tanto en TRAINING COMO EN VALIDATION, las labels se generan por desplazamiento de un batch hacia delante.\n",
    "\n",
    "\n",
    "#DAF: Las funciones a continuacion se usan cada 1000 steps de Training para pintar 5 muestras de secuencias de 80 letras\n",
    "#Las secuencias las genera el MODELO con la Accuracy alcanzada a esa altura del training. Se supone que cada vez \n",
    "#las secuencias serán menos al azar y más coherentes.\n",
    "#La eleccion de la primera letra de cada secuencia se hace al azar (sample(random_distribution())). Para asegurarlo\n",
    "#luego se resetea el estado de la red (reset_sample_state.run())\n",
    "#Las siguientes 79 elecciones son PREDICCIONES BASADAS EN EL ESTADO DE LA RED, \n",
    "#QUE VA TENIENDO EN CUENTA TODAS LAS LETRAS ANTERIORES ELEGIDAS MEDIANTE LA ACTUALIZACION DEL ESTADO.\n",
    "#for _ in range(79):\n",
    "#  prediction = sample_prediction.eval({sample_input: feed})\n",
    "#  feed = sample(prediction)\n",
    "#  sentence += characters(feed)[0]\n",
    "#\n",
    "#DAF: En sample_prediction, definida en el Graph, se ejecutan todas las operaciones previas para realizar una estimación \n",
    "#basada en el estado \"hasta ahora\" de la red (saved_sample_state) asi como la outputs previos (saved_sample_output). VEASE\n",
    "\n",
    "\n",
    "def sample_distribution(distribution):  #normalized probabilities: que todas suman 1\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction): #parece que distibution esta en prediction[0], es la primera columna\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float) \n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p #p.shape=[1,27]\n",
    "\n",
    "def random_distribution(): #las normaliza en el return\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None] #shape=(1,27)\n",
    "\n",
    "#DAF understanding\n",
    "d= random_distribution()\n",
    "print(d)\n",
    "print(sum(d[0]))\n",
    "print(sample(d))\n",
    "\n",
    "#DAF: random_distribution() devuelve un PREDICTION: un vector de PROBABILIDADES normalizadas (suman 1) de shape=[1,27]. \n",
    "#Se puede por ejemplo confrontar a un vector de LABELS de shape=[27]. \n",
    "#sample(random_distribution()), devuelve 1 letra with hot enconding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat: Tensor(\"concat_4:0\", shape=(640, 64), dtype=float32) w: <tf.Variable 'Variable_16:0' shape=(64, 27) dtype=float32_ref> b: <tf.Variable 'Variable_17:0' shape=(27,) dtype=float32_ref>\n",
      "logits: Tensor(\"xw_plus_b:0\", shape=(640, 27), dtype=float32)\n",
      "labels: Tensor(\"concat_5:0\", shape=(640, 27), dtype=float32)\n",
      "cross-entropy: Tensor(\"Reshape_5:0\", shape=(640,), dtype=float32)\n",
      "train_prediction:  Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      "sample_prediction: Tensor(\"Softmax_1:0\", shape=(1, 27), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  #DAF understanding: PROBLEM 6.1. OJO: QUITAR LUEGO\n",
    "  i = tf.Variable(tf.truncated_normal([num_nodes,vocabulary_size], -0.1, 0.1)) #shape=[64,27]\n",
    "  o = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1)) #shape=[64,64]\n",
    "\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1)) # _x.shape=[27,64]\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1)) # _m.shape=[64,64] Wi\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes])) # _b.shape=[1,64]\n",
    "    \n",
    "  # Forget gate: input, previous output, and bias. # Iguales shapes\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1)) # _x.shape=[27,64]\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1)) # _m.shape=[64,64] Wf\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes])) # _b.shape=[1,64]\n",
    "    \n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1)) # _x.shape=[27,64]\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1)) # STATE DE CELL _m.shape=[64,64]\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes])) # _b.shape=[1,64]\n",
    "    \n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1)) # _x.shape=[27,64]\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1)) # _m.shape=[64,64] Wo\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes])) # _b.shape=[1,64]\n",
    "    \n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  #DAF: TO UNDERSTAND\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) # saved_output.shape=[64,64]\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) # saved_state.shape=[64,64]\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  #DAF: TO UNDERSTAND\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1)) # w.shape=[64,27]\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size])) # b.shape =[27,]\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state): #i:input shape=[64,27], o:saved_output (shape=[64,64]), state:saved_state (shape=[64,64])\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib) #Gate: le alimentan i=input y o=saved_output\n",
    "    \n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb) #Gate: le alimentan i=input y o=saved_output\n",
    "    \n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb #Cell:  le alimentan i=input y o=saved_output, generan:-> update\n",
    "    \n",
    "    state = forget_gate * state + input_gate * tf.tanh(update) #el nuevo state (se devolverá)\n",
    "    \n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob) #Gate: le alimentan i=input y o=saved_output.\n",
    "    #DAF: esta linea se podria subir sin verse afectado nada\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state  # output_gate * tf.tanh(state) es el OUTPUT (recurrent or previous output)\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append( \n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size])) #se le añaden 11 batch de (shape=[64,27])\n",
    "\n",
    "  train_inputs = train_data[:num_unrollings] #hasta el decimo, el ultimo no entra\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step. #del segundo al ultimo\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output #shape=[64,64]\n",
    "  state = saved_state #shape=[64,64]\n",
    "    \n",
    "  for i in train_inputs: #10 x shape=[64,27]\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    #DAF understanding\n",
    "    #print(output)\n",
    "    outputs.append(output) #se van guardando los 10 outputs, todos generados por lstm_cell \n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "    #DAF: el calculo de los logits y da la loss se hace habiendo previamente salvado los ultimos output y state    \n",
    "    #como saved_output y saved_state\n",
    "    \n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # tf.concat(outputs, 0) shape=(640, 64)\n",
    "    # w shape=(64, 27), b shape=(27,)\n",
    "    # tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) --> matmul(x, w) + b , \n",
    "    # logits shape=(640, 27)\n",
    "    \n",
    "    \n",
    "    loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits) )\n",
    "    # labels shape=(640, 27)\n",
    "    # xxxx_cross_entropy shape=(640,)\n",
    "  \n",
    "  #DAF understanding\n",
    "  print(\"concat:\", tf.concat(outputs, 0), \"w:\", w, \"b:\", b)\n",
    "  print(\"logits:\", logits)\n",
    "  print(\"labels:\", tf.concat(train_labels, 0))\n",
    "  print(\"cross-entropy:\", tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True) #Desde el step 5000, 10.0/0.1=1.0\n",
    "    \n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss)) \n",
    "  #This is the first part of minimize(). \n",
    "  #It returns a list of (gradient, variable) pairs where \"gradient\" is the gradient for \"variable\". \n",
    "\n",
    "  #DAF: entremedias se hace el \"Gradient Clipping\"  \n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25) #clipp norm=1.25, si la normal de los gradients crece demasiado se aplica esta  \n",
    "\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step) \n",
    "  #This is the second part of minimize(). It returns an Operation that applies gradients.\n",
    "  #grads_and_vars: List of (gradient, variable) pairs as returned by compute_gradients()\n",
    "\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  print(\"train_prediction: \",train_prediction)\n",
    "    \n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #DAF: NO UNROLLING SOLO 1 VEZ, Y BATCH_SIZE=1. ESTA PARTE SE USA UNICAMENTE EN LOS RESUMENES: EN SAMPLING (cada 1000 steps)\n",
    "  #Y EN EL CALCULO DE PERPLEXITY CONTRA VALIDATION SET \n",
    "\n",
    "  #DAF: sample_input: en la session lo alimenta feed:\n",
    "  #- En el Sampling de Secuencias con 1 letra al azar la primera vez y luego con la actual ultima generada en bucle 0-79\n",
    "  #- Contra el Validation Set. Los batches son 2 batch x 1letra: el feed es la 1ºletra y la 2ª es la valid_label\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size]) #shape=[1,27]\n",
    "\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes])) #shape=[1,64]\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes])) #shape=[1,64]\n",
    "  \n",
    "  #\n",
    "  reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), \n",
    "                                saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  #DAF: reset_sample_state no tiene que ver con la siguiente linea, se llama en la session:\n",
    "  # - al comienzo del Sampling cada una de las 5 secuencias sampleadas (cada 1000 steps).\n",
    "  # - antes de calcular la Perplexity contra el Validation Set (cada 1000 steps) \n",
    "  \n",
    "  #DAF: Una sola llamada (sin bucle) a lstm_cell pues con sample_input de [1,27]\n",
    "  # y saved_sample_output, saved_sample_state de [1,64]\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "\n",
    "  #DAF: el calculo sample_prediction se hace habiendo previamente salvado los ultimos sample_output y sample_state    \n",
    "  #como saved_sample_output y saved_sample_state\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):        \n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b)) #1 letra shape=(1, 27)\n",
    "    \n",
    "  print(\"sample_prediction:\", sample_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len de batches: 11 batches[0].shape: (64, 27)  labels.shape: (640, 27)\n",
      "lpm.shape (640, 27)\n",
      "Minibatch perplexity: 2.69\n",
      "[[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "feed shape (1, 27)\n",
      "sentence: m\n"
     ]
    }
   ],
   "source": [
    "#DAF understanding of Session\n",
    "batches = train_batches.next()\n",
    "labels = np.concatenate(list(batches)[1:])\n",
    "print(\"len de batches:\", len(batches), \"batches[0].shape:\", batches[0].shape, \" labels.shape:\", labels.shape)\n",
    "_predictions = np.random.uniform(0.0, 1.0, size=[640, 27])\n",
    "lpm = np.multiply(labels, -np.log(_predictions))\n",
    "print (\"lpm.shape\", lpm.shape)\n",
    "print('Minibatch perplexity: %.2f' % float(np.exp(logprob(_predictions, labels))))\n",
    "feed = sample(random_distribution()) #1 letra en 1hot-encoding\n",
    "print(feed)\n",
    "print(\"feed shape\", feed.shape)\n",
    "for _ in range(5):\n",
    "  feed = sample(random_distribution())\n",
    "  sentence = characters(feed)[0]\n",
    "print(\"sentence:\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295013 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "cd o    yixe rsfez t xkeim utn c gsmn o swwytc cyvnlyqhtgtttkssxe ysymrmnetoclxd\n",
      "sorodwx uejsqient f ko e werbxn  wmlmhsfdmz rj zaeyyeksswy orsvbzls     vfrtk  k\n",
      "kvkneannm trviqkwcj rtzs fykoisrpiixutx jiyotsgyqq rxeed  ovdsejtrwqah  yurm psb\n",
      "gzlcnrcnqkowp e qibbc mbl z ve  ey  evnlus unha prttedsd i ra srle ser e lrviftm\n",
      "yrhtsl vmo ozszekkszununjxang njggpjs s j  dxibch eoeejvel cjokz n  orn ynloejvu\n",
      "================================================================================\n",
      "Validation set perplexity: 20.12\n",
      "Average loss at step 100: 2.590954 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.08\n",
      "Validation set perplexity: 12.04\n",
      "Average loss at step 200: 2.245588 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.39\n",
      "Validation set perplexity: 8.66\n",
      "Average loss at step 300: 2.100155 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 7.88\n",
      "Average loss at step 400: 2.003907 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.78\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 500: 1.936739 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 600: 1.918075 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 700: 1.866832 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 800: 1.829238 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 900: 1.839824 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 1000: 1.817325 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "================================================================================\n",
      "ring kambational lebilar petsiones the fismonotny of wasd histolmovically hindik\n",
      "bonic composed to oric dist nine sibur trevs line from servers pasitil gay han r\n",
      "s are a tlo threm his efigial in uce econaly g tate divinumation ratauts appicti\n",
      "e the six it irkevcister he may sebrov beenipres speearisle dicha irtions in a r\n",
      "on proigoring impgeate iw dising dut c oppown relegonations of tide opened esten\n",
      "================================================================================\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 1100: 1.774809 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1200: 1.762257 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1300: 1.736976 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1400: 1.755713 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1500: 1.738372 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 1600: 1.750208 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1700: 1.714564 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1800: 1.673136 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 1900: 1.659797 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2000: 1.703994 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      " one nine nine four two from then gefer zero two four menstory is had been nine \n",
      "jaation words lace of the goded holled coust tory the continum from the word the\n",
      "vets contommef f comple the has they things imperia to mode voint see acton pros\n",
      "kilitical whatairs a fromn by schabinic implenty and to communed en compacitioni\n",
      "mack and ofle t nualusian and insaves featcode them wsssem to the oundor office \n",
      "================================================================================\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2100: 1.691778 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2200: 1.680907 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2300: 1.652521 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 2400: 1.666426 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2500: 1.676276 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2600: 1.658901 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2700: 1.660723 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2800: 1.651282 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 2900: 1.651764 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3000: 1.647282 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "ated only by repultable beins progranchrathre beivel an begain harerisms janzy f\n",
      "what should six a deside although tasro resortes becala fagire have occulicity a\n",
      "ar secovolcagator murzer to reverionize right not aspandilor the poputatish dosc\n",
      "x are theser ban an eright his a uses in mostall and froman mores an uns onler i\n",
      "stars bexomic know with rineay groglturo one is the mence by sult to mays then s\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3100: 1.642934 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3200: 1.638245 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3300: 1.655946 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3400: 1.664282 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3500: 1.655266 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3600: 1.667863 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 3700: 1.647498 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3800: 1.640041 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3900: 1.637264 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4000: 1.646390 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "e the in repleeder loaur cult whiles with with vickene and more to execture of t\n",
      "zation v d soue from reeina of the pprongram of floment pcul friczon usually nor\n",
      "ion number that out of luctiofs manys pride renomic is adigers or motor the reas\n",
      "fino beronvic celesciplly whut the finstory of ismetion tyak are the still made \n",
      "y opening of litur moth renation endlevingtory and open the trigens for eniograi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4100: 1.638144 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4200: 1.630894 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4300: 1.610880 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4400: 1.619351 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4500: 1.616415 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4600: 1.620972 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4700: 1.621376 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4800: 1.637220 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4900: 1.633566 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5000: 1.604653 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "================================================================================\n",
      "onomes infoys which app ws expressems in the such the gruke viing the polity peo\n",
      "galis eathers wide the basleks of the his sharenc schodege thing chesti but when\n",
      "geprenny spelle wavell dums his for but checkatil her this wrothin the sain with\n",
      "noed over sporth axame s the providen this other liphor cariginal esturity s ahs\n",
      "ard all contrifes of wintanioball its to developticularly of the dividuet trame \n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5100: 1.601427 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5200: 1.596396 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5300: 1.572637 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.05\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5400: 1.575175 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5500: 1.573317 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5600: 1.582537 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5700: 1.568425 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5800: 1.571352 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5900: 1.576728 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6000: 1.554234 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "rijutives monenct rebamout old sly pertion her easher that one nine three city a\n",
      "jing on a tradures were proposlation have mimitinary infacomether genoboby kempe\n",
      "quey one nine one zero valiism that is willing of the is an economic gravition t\n",
      "imily extames a parnacially for one six nine mag king a b the filies sty scipen \n",
      "mail by american high slovariex duke spacire recur from of byphiate ouriden eigh\n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6100: 1.560109 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6200: 1.543023 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6300: 1.535062 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6400: 1.546027 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6500: 1.564359 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6600: 1.594863 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6700: 1.594256 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6800: 1.598966 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6900: 1.581736 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 7000: 1.582356 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "barraniess of incluse american extart school who plosed the hader france has res\n",
      "old occurnetus seventing dix airpics of in the side presigned expends of the puk\n",
      "al folly which two zero nige the easly and used in the onch chabled a referret i\n",
      "ricands no iphather high ofton concomes geol for the kechnic surrecoriant actric\n",
      "acks of the one zero play death stanged or malmecturation and best a seasly one \n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "\n",
    "  for step in range(num_steps):\n",
    "\n",
    "    batches = train_batches.next() #DAF: len de batches: 11 batch. shape de cada batch: (64, 27) (64 letras)\n",
    "    #DAF: segun batches2string(batches), \n",
    "    feed_dict = dict()   \n",
    "    for i in range(num_unrollings + 1): #i va de 0 a 11\n",
    "      feed_dict[train_data[i]] = batches[i] \n",
    "    \n",
    "    #DAF: CLAVE train_data es una lista de 11 tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size])) \n",
    "    #a train_data[i] se le asigna un batch (batches[i]) de 64 letras, shape=(64,27)\n",
    "    #\n",
    "    #tal y como se define en el Graph de train_data, se extraeran:\n",
    "    #train_inputs = train_data[0:10]\n",
    "    #train_labels = train_data[1:11]\n",
    "    #\n",
    "    #los train_inputs alimentan como input los 10 unrollings de la red (en cada step):\n",
    "    #for i in train_inputs: output, state = lstm_cell(i, output, state)\n",
    "    #y los train_labels se usaran como \"labels\" de softmax_cross_entropy_with_logits() para calcular la loss\n",
    "    #\n",
    "\n",
    "    \n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    \n",
    "    #DAF: como siempre train_prediction = tf.nn.softmax(logits)\n",
    "    #(y los logits se sacan a partir de la lista de 10 outputs de lstm_cell:\n",
    "    #aplicandoles un linear classifier: logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) \n",
    "    \n",
    "    mean_loss += l\n",
    "    \n",
    "    if step % summary_frequency == 0:\n",
    "        \n",
    "      if step > 0: \n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0 #DAF: ...sobre los ultimos 100\n",
    "    \n",
    "      labels = np.concatenate(list(batches)[1:]) #DAF: labels.shape=[640,27]\n",
    "      #DAF: predictions = train_prediction = tf.nn.softmax(logits), predictions.shape=(640,27)        \n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels)))) \n",
    "    \n",
    "      if step % (summary_frequency * 10) == 0: #DAF: cada 1000 steps\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution()) #feed es una letra sampleada [1,27]. La primera de cada seq al azar\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "      \n",
    "          #DAF: reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), \n",
    "          #saved_sample_state.assign(tf.zeros([1, num_nodes]))). Se hace al comienzo de cada seq generada\n",
    "            \n",
    "          #DAF: CLAVE: aqui a partir del sample (el ultimo generado=sample) se generan 5 sentencias de len=80, usando la Network\n",
    "    \n",
    "          for _ in range(79):\n",
    "            #DAF feed alimenta, via la var sample input, el proceso de sample_prediction\n",
    "            #y luego se vuelve a samplear un nuevo feed (una nueva letra), basado en la prediccion de sample_prediction \n",
    "            prediction = sample_prediction.eval({sample_input: feed}) \n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "            \n",
    "          print(sentence)  \n",
    "        print('=' * 80)\n",
    "        \n",
    "      \n",
    "      #DAF: EL CALCULO DE LA PERPLEXITY CONTRA EL VALIDATION SET DE 1000 SAMPLES (se hace cada 100 steps)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "    \n",
    "      for _ in range(valid_size): #DAF: valid_size = 1000, valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "        b = valid_batches.next() #b.shape=[1,1+1=2]\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]}) #se hace la prediccion con el primer caracter\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1]) # la label es el segundo caracter\n",
    "  \n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    " \n",
    "\n",
    "  # Parameters:\n",
    "  # All the gates: input, previous output, and bias.\n",
    "  cix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1)) # cix.shape=[27, 256]\n",
    "  cim = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1)) # cim.shape=[64, 256]\n",
    "  cib = tf.Variable(tf.zeros([1, num_nodes * 4])) # cib.shape=[1,256]\n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) # saved_output.shape=[64,64]\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) # saved_state.shape=[64,64]\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1)) # w.shape=[64,27]\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size])) # b.shape =[27,]\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state): #i:input shape=[64,27], o:saved_output (shape=[64,64]), state:saved_state (shape=[64,64])\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    all_gates = tf.matmul(i, cix) + tf.matmul(o, cim) + cib # all_gates.shape=[64, 256]\n",
    "    #  matmul(i.shape=[64, 27],cix.shape=[27, 256]) --> matmul.shape=[64, 256]\n",
    "    #+ matmul(o.shape=[64,64], cim.shape=[64, 256]) --> matmul.shape=[64, 256], +.shape=[64, 256]\n",
    "    #+ cib.shape=[1, 256] -->  all_gates.shape=[64, 256]\n",
    "    \n",
    "    input_gate = tf.sigmoid(all_gates[:, 0:num_nodes])\n",
    "    \n",
    "    forget_gate = tf.sigmoid(all_gates[:, num_nodes:2*num_nodes])\n",
    "    \n",
    "    update = all_gates[:, 2*num_nodes:3*num_nodes]\n",
    "    \n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    \n",
    "    output_gate = tf.sigmoid(all_gates[:, 3*num_nodes:])\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append( \n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output #shape=[64,64]\n",
    "  state = saved_state #shape=[64,64]\n",
    "    \n",
    "  for i in train_inputs: #10 x shape=[64,27]\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    \n",
    "    loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits) )\n",
    "  \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    \n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss)) \n",
    "  #This is the first part of minimize(). \n",
    "  #It returns a list of (gradient, variable) pairs where \"gradient\" is the gradient for \"variable\". \n",
    "\n",
    "  #DAF: entremedias se hace el \"Gradient Clipping\"  \n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step) \n",
    "  #This is the second part of minimize(). It returns an Operation that applies gradients.\n",
    "  #grads_and_vars: List of (gradient, variable) pairs as returned by compute_gradients()\n",
    "\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size]) #shape=[1,27]\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes])) #shape=[1,64]\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes])) #shape=[1,64]\n",
    "  \n",
    "  reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), \n",
    "                                saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):        \n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b)) #1 letra shape=(1, 27)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297096 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.03\n",
      "================================================================================\n",
      "yrkh lpu s turq vtuhdfahclndcuhy xecrkyi azfq jagi tmrns rpfdnpdewuduvaelcrpwmtm\n",
      "atlskgstytbn lok o djagkiaedqrlqxnfqqtsu glaanwor e zeg m t ell khljnt  umerou  \n",
      " nhteid bwayarkmkhei dl oakmoeby usrisib iqvveoiofvj a afvgae nlynttmwrnt sji dz\n",
      "gao lxri hsfte knaefsotilcj i ujbncako  jm m mzowckhnh yfdjg s j zoeaqsrkessid i\n",
      "iaw  rinnxis yjfqpe antrszvodnd azgeppnpvni  qectaurvyqrsvrm ueqmdgfqv vu mvdnot\n",
      "================================================================================\n",
      "Validation set perplexity: 20.06\n",
      "Average loss at step 100: 2.587039 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.23\n",
      "Validation set perplexity: 10.88\n",
      "Average loss at step 200: 2.243460 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.65\n",
      "Validation set perplexity: 9.20\n",
      "Average loss at step 300: 2.096400 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.61\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 400: 1.998717 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.67\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 500: 1.933774 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 600: 1.903830 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 700: 1.855852 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 800: 1.823517 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 900: 1.832081 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.29\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1000: 1.825473 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "================================================================================\n",
      "ation sign of disturins an or farieity was the it is was be infession difforvati\n",
      "kish ces in fiven hreeczed ted utiming chering alcic moncerin litfes siccer laci\n",
      "d kar areressly chix disentist chribits cauted and which reaptully distric disti\n",
      "quaderitiona in co preardaty and pirsounce and proling is section ild tice etern\n",
      "mance if which spurisvion sely ederced in the delic diviration manivity the prep\n",
      "================================================================================\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1100: 1.776825 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.84\n"
     ]
    }
   ],
   "source": [
    "#num_steps = 7001\n",
    "num_steps = 1101\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "\n",
    "  for step in range(num_steps):\n",
    "\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()   \n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i] \n",
    "    \n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    \n",
    "    if step % summary_frequency == 0:\n",
    "        \n",
    "      if step > 0: \n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "    \n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels)))) \n",
    "    \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution()) #feed es una letra sampleada [1,27]. La primera de cada seq al azar\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          \n",
    "          for _ in range(79):\n",
    "            #DAF feed alimenta, via la var sample input, el proceso de sample_prediction\n",
    "            #y luego se vuelve a samplear un nuevo feed (una nueva letra), basado en la prediccion de sample_prediction \n",
    "            prediction = sample_prediction.eval({sample_input: feed}) \n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "            \n",
    "          print(sentence)  \n",
    "        print('=' * 80)\n",
    "        \n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "    \n",
    "      for _ in range(valid_size): #DAF: valid_size = 1000, valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "        b = valid_batches.next() #b.shape=[1,1+1=2]\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]}) #se hace la prediccion con el primer caracter\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1]) # la label es el segundo caracter\n",
    "  \n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(on)(ns)(s )( a)(an)(na)(ar)(rc)(ch)(hi)(is)', '(wh)(he)(en)(n )( m)(mi)(il)(li)(it)(ta)(ar)', '(ll)(le)(er)(ri)(ia)(a )( a)(ar)(rc)(ch)(he)', '( a)(ab)(bb)(be)(ey)(ys)(s )( a)(an)(nd)(d )', '(ma)(ar)(rr)(ri)(ie)(ed)(d )( u)(ur)(rr)(ra)', '(he)(el)(l )( a)(an)(nd)(d )( r)(ri)(ic)(ch)', '(y )( a)(an)(nd)(d )( l)(li)(it)(tu)(ur)(rg)', '(ay)(y )( o)(op)(pe)(en)(ne)(ed)(d )( f)(fo)', '(ti)(io)(on)(n )( f)(fr)(ro)(om)(m )( t)(th)', '(mi)(ig)(gr)(ra)(at)(ti)(io)(on)(n )( t)(to)', '(ne)(ew)(w )( y)(yo)(or)(rk)(k )( o)(ot)(th)', '(he)(e )( b)(bo)(oe)(ei)(in)(ng)(g )( s)(se)', '(e )( l)(li)(is)(st)(te)(ed)(d )( w)(wi)(it)', '(eb)(be)(er)(r )( h)(ha)(as)(s )( p)(pr)(ro)', '(o )( b)(be)(e )( m)(ma)(ad)(de)(e )( t)(to)', '(ye)(er)(r )( w)(wh)(ho)(o )( r)(re)(ec)(ce)', '(or)(re)(e )( s)(si)(ig)(gn)(ni)(if)(fi)(ic)', '(a )( f)(fi)(ie)(er)(rc)(ce)(e )( c)(cr)(ri)', '( t)(tw)(wo)(o )( s)(si)(ix)(x )( e)(ei)(ig)', '(ar)(ri)(is)(st)(to)(ot)(tl)(le)(e )( s)(s )', '(it)(ty)(y )( c)(ca)(an)(n )( b)(be)(e )( l)', '( a)(an)(nd)(d )( i)(in)(nt)(tr)(ra)(ac)(ce)', '(ti)(io)(on)(n )( o)(of)(f )( t)(th)(he)(e )', '(dy)(y )( t)(to)(o )( p)(pa)(as)(ss)(s )( h)', '(f )( c)(ce)(er)(rt)(ta)(ai)(in)(n )( d)(dr)', '(at)(t )( i)(it)(t )( w)(wi)(il)(ll)(l )( t)', '(e )( c)(co)(on)(nv)(vi)(in)(nc)(ce)(e )( t)', '(en)(nt)(t )( t)(to)(ol)(ld)(d )( h)(hi)(im)', '(am)(mp)(pa)(ai)(ig)(gn)(n )( a)(an)(nd)(d )', '(rv)(ve)(er)(r )( s)(si)(id)(de)(e )( s)(st)', '(io)(ou)(us)(s )( t)(te)(ex)(xt)(ts)(s )( s)', '(o )( c)(ca)(ap)(pi)(it)(ta)(al)(li)(iz)(ze)', '(a )( d)(du)(up)(pl)(li)(ic)(ca)(at)(te)(e )', '(gh)(h )( a)(an)(nn)(n )( e)(es)(s )( d)(d )', '(in)(ne)(e )( j)(ja)(an)(nu)(ua)(ar)(ry)(y )', '(ro)(os)(ss)(s )( z)(ze)(er)(ro)(o )( t)(th)', '(ca)(al)(l )( t)(th)(he)(eo)(or)(ri)(ie)(es)', '(as)(st)(t )( i)(in)(ns)(st)(ta)(an)(nc)(ce)', '( d)(di)(im)(me)(en)(ns)(si)(io)(on)(na)(al)', '(mo)(os)(st)(t )( h)(ho)(ol)(ly)(y )( m)(mo)', '(t )( s)(s )( s)(su)(up)(pp)(po)(or)(rt)(t )', '(u )( i)(is)(s )( s)(st)(ti)(il)(ll)(l )( d)', '(e )( o)(os)(sc)(ci)(il)(ll)(la)(at)(ti)(in)', '(o )( e)(ei)(ig)(gh)(ht)(t )( s)(su)(ub)(bt)', '(of)(f )( i)(it)(ta)(al)(ly)(y )( l)(la)(an)', '(s )( t)(th)(he)(e )( t)(to)(ow)(we)(er)(r )', '(kl)(la)(ah)(ho)(om)(ma)(a )( p)(pr)(re)(es)', '(er)(rp)(pr)(ri)(is)(se)(e )( l)(li)(in)(nu)', '(ws)(s )( b)(be)(ec)(co)(om)(me)(es)(s )( t)', '(et)(t )( i)(in)(n )( a)(a )( n)(na)(az)(zi)', '(th)(he)(e )( f)(fa)(ab)(bi)(ia)(an)(n )( s)', '(et)(tc)(ch)(hy)(y )( t)(to)(o )( r)(re)(el)', '( s)(sh)(ha)(ar)(rm)(ma)(an)(n )( n)(ne)(et)', '(is)(se)(ed)(d )( e)(em)(mp)(pe)(er)(ro)(or)', '(ti)(in)(ng)(g )( i)(in)(n )( p)(po)(ol)(li)', '(d )( n)(ne)(eo)(o )( l)(la)(at)(ti)(in)(n )', '(th)(h )( r)(ri)(is)(sk)(ky)(y )( r)(ri)(is)', '(en)(nc)(cy)(yc)(cl)(lo)(op)(pe)(ed)(di)(ic)', '(fe)(en)(ns)(se)(e )( t)(th)(he)(e )( a)(ai)', '(du)(ua)(at)(ti)(in)(ng)(g )( f)(fr)(ro)(om)', '(tr)(re)(ee)(et)(t )( g)(gr)(ri)(id)(d )( c)', '(at)(ti)(io)(on)(ns)(s )( m)(mo)(or)(re)(e )', '(ap)(pp)(pe)(ea)(al)(l )( o)(of)(f )( d)(de)', '(si)(i )( h)(ha)(av)(ve)(e )( m)(ma)(ad)(de)']\n",
      "['(is)(st)(ts)(s )( a)(ad)(dv)(vo)(oc)(ca)(at)', '(ar)(ry)(y )( g)(go)(ov)(ve)(er)(rn)(nm)(me)', '(he)(es)(s )( n)(na)(at)(ti)(io)(on)(na)(al)', '(d )( m)(mo)(on)(na)(as)(st)(te)(er)(ri)(ie)', '(ra)(ac)(ca)(a )( p)(pr)(ri)(in)(nc)(ce)(es)', '(ch)(ha)(ar)(rd)(d )( b)(ba)(ae)(er)(r )( h)', '(rg)(gi)(ic)(ca)(al)(l )( l)(la)(an)(ng)(gu)', '(fo)(or)(r )( p)(pa)(as)(ss)(se)(en)(ng)(ge)', '(th)(he)(e )( n)(na)(at)(ti)(io)(on)(na)(al)', '(to)(oo)(ok)(k )( p)(pl)(la)(ac)(ce)(e )( d)', '(th)(he)(er)(r )( w)(we)(el)(ll)(l )( k)(kn)', '(se)(ev)(ve)(en)(n )( s)(si)(ix)(x )( s)(se)', '(it)(th)(h )( a)(a )( g)(gl)(lo)(os)(ss)(s )', '(ro)(ob)(ba)(ab)(bl)(ly)(y )( b)(be)(ee)(en)', '(to)(o )( r)(re)(ec)(co)(og)(gn)(ni)(iz)(ze)', '(ce)(ei)(iv)(ve)(ed)(d )( t)(th)(he)(e )( f)', '(ic)(ca)(an)(nt)(t )( t)(th)(ha)(an)(n )( i)', '(ri)(it)(ti)(ic)(c )( o)(of)(f )( t)(th)(he)', '(ig)(gh)(ht)(t )( i)(in)(n )( s)(si)(ig)(gn)', '(s )( u)(un)(nc)(ca)(au)(us)(se)(ed)(d )( c)', '( l)(lo)(os)(st)(t )( a)(as)(s )( i)(in)(n )', '(ce)(el)(ll)(lu)(ul)(la)(ar)(r )( i)(ic)(ce)', '(e )( s)(si)(iz)(ze)(e )( o)(of)(f )( t)(th)', '( h)(hi)(im)(m )( a)(a )( s)(st)(ti)(ic)(ck)', '(dr)(ru)(ug)(gs)(s )( c)(co)(on)(nf)(fu)(us)', '( t)(ta)(ak)(ke)(e )( t)(to)(o )( c)(co)(om)', '( t)(th)(he)(e )( p)(pr)(ri)(ie)(es)(st)(t )', '(im)(m )( t)(to)(o )( n)(na)(am)(me)(e )( i)', '(d )( b)(ba)(ar)(rr)(re)(ed)(d )( a)(at)(tt)', '(st)(ta)(an)(nd)(da)(ar)(rd)(d )( f)(fo)(or)', '( s)(su)(uc)(ch)(h )( a)(as)(s )( e)(es)(so)', '(ze)(e )( o)(on)(n )( t)(th)(he)(e )( g)(gr)', '(e )( o)(of)(f )( t)(th)(he)(e )( o)(or)(ri)', '(d )( h)(hi)(iv)(ve)(er)(r )( o)(on)(ne)(e )', '(y )( e)(ei)(ig)(gh)(ht)(t )( m)(ma)(ar)(rc)', '(th)(he)(e )( l)(le)(ea)(ad)(d )( c)(ch)(ha)', '(es)(s )( c)(cl)(la)(as)(ss)(si)(ic)(ca)(al)', '(ce)(e )( t)(th)(he)(e )( n)(no)(on)(n )( g)', '(al)(l )( a)(an)(na)(al)(ly)(ys)(si)(is)(s )', '(mo)(or)(rm)(mo)(on)(ns)(s )( b)(be)(el)(li)', '(t )( o)(or)(r )( a)(at)(t )( l)(le)(ea)(as)', '( d)(di)(is)(sa)(ag)(gr)(re)(ee)(ed)(d )( u)', '(in)(ng)(g )( s)(sy)(ys)(st)(te)(em)(m )( e)', '(bt)(ty)(yp)(pe)(es)(s )( b)(ba)(as)(se)(ed)', '(an)(ng)(gu)(ua)(ag)(ge)(es)(s )( t)(th)(he)', '(r )( c)(co)(om)(mm)(mi)(is)(ss)(si)(io)(on)', '(es)(ss)(s )( o)(on)(ne)(e )( n)(ni)(in)(ne)', '(nu)(ux)(x )( s)(su)(us)(se)(e )( l)(li)(in)', '( t)(th)(he)(e )( f)(fi)(ir)(rs)(st)(t )( d)', '(zi)(i )( c)(co)(on)(nc)(ce)(en)(nt)(tr)(ra)', '( s)(so)(oc)(ci)(ie)(et)(ty)(y )( n)(ne)(eh)', '(el)(la)(at)(ti)(iv)(ve)(el)(ly)(y )( s)(st)', '(et)(tw)(wo)(or)(rk)(ks)(s )( s)(sh)(ha)(ar)', '(or)(r )( h)(hi)(ir)(ro)(oh)(hi)(it)(to)(o )', '(li)(it)(ti)(ic)(ca)(al)(l )( i)(in)(ni)(it)', '(n )( m)(mo)(os)(st)(t )( o)(of)(f )( t)(th)', '(is)(sk)(ke)(er)(rd)(do)(oo)(o )( r)(ri)(ic)', '(ic)(c )( o)(ov)(ve)(er)(rv)(vi)(ie)(ew)(w )', '(ai)(ir)(r )( c)(co)(om)(mp)(po)(on)(ne)(en)', '(om)(m )( a)(ac)(cn)(nm)(m )( a)(ac)(cc)(cr)', '( c)(ce)(en)(nt)(te)(er)(rl)(li)(in)(ne)(e )', '(e )( t)(th)(ha)(an)(n )( a)(an)(ny)(y )( o)', '(de)(ev)(vo)(ot)(ti)(io)(on)(na)(al)(l )( b)', '(de)(e )( s)(su)(uc)(ch)(h )( d)(de)(ev)(vi)']\n",
      "['( a)(an)']\n",
      "['(an)(na)']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "\n",
    "\n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current bigram positions in the data. The bigrams are Idx (an embedding)\"\"\"\n",
    "    batch = list()\n",
    "    for b in range(self._batch_size):\n",
    "      first_char = self._text[self._cursor[b]]\n",
    "\n",
    "      if self._cursor[b] + 1 == self._text_size:\n",
    "        second_char = ' '\n",
    "      else:\n",
    "        second_char = self._text[self._cursor[b] + 1]\n",
    "      \n",
    "      batch.append(char2id(first_char) * vocabulary_size + char2id(second_char))\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "\n",
    "\n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "\n",
    "bigram_train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "bigram_valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "#DAF valid num_unrollings=1 una sola llamada a lstm_cell, y una sola actualizacion de state y output\n",
    "\n",
    "def characters_from_embed(embeddings):\n",
    "  r = [ '(' + id2char(e//vocabulary_size) + id2char(e%vocabulary_size) + ')' for e in embeddings]\n",
    "  return r\n",
    "\n",
    "def bigram_characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return ['({0},{1})'.format(id2char(c//vocabulary_size), id2char(c % vocabulary_size))\n",
    "          for c in np.argmax(probabilities,1)]\n",
    "\n",
    "\n",
    "def bigram_batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * len(batches[0])\n",
    "  for b in batches:\n",
    "    #print(\"b:\",b)\n",
    "    #print(\"bigram:\", bigram_characters(b))\n",
    "    s = [''.join(x) for x in zip(s, characters_from_embed(b))]\n",
    "  return s\n",
    "\n",
    "print(bigram_batches2string(bigram_train_batches.next()))\n",
    "print(bigram_batches2string(bigram_train_batches.next()))\n",
    "print(bigram_batches2string(bigram_valid_batches.next()))\n",
    "print(bigram_batches2string(bigram_valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.44825698e-04   1.07784967e-03   2.55285164e-03   6.99662113e-04\n",
      "    1.78662810e-03   1.84302053e-03   2.19808691e-03   1.22244053e-03\n",
      "    2.15345549e-04   4.92993521e-05   9.94352749e-04   1.87747960e-04\n",
      "    5.27131785e-04   1.07772411e-03   3.90018864e-04   1.19175427e-03\n",
      "    7.10423578e-04   1.88689320e-03   1.16460165e-03   4.74822341e-04\n",
      "    1.96917747e-03   1.05090196e-03   1.35940722e-03   1.13100011e-03\n",
      "    7.27387067e-04   1.07568233e-04   2.76806607e-04   1.36206079e-03\n",
      "    5.84003512e-04   1.45150692e-03   3.80752226e-04   2.45719465e-03\n",
      "    3.26719181e-04   3.92169274e-04   8.80921418e-04   7.67155825e-04\n",
      "    1.57438244e-03   1.35749739e-03   1.45031078e-03   1.49436242e-03\n",
      "    3.42735174e-04   1.00775809e-03   5.85305703e-04   2.67090304e-03\n",
      "    2.53403321e-03   5.48576807e-04   2.50746291e-03   2.32701138e-04\n",
      "    2.03984656e-03   2.67804403e-04   2.23301463e-03   1.85972591e-03\n",
      "    1.21422080e-04   2.64197061e-03   1.79395747e-03   8.05613606e-04\n",
      "    6.50489955e-04   1.81954233e-03   2.97948826e-04   3.95438818e-04\n",
      "    1.33897544e-03   2.44576937e-03   2.25152564e-03   1.57472909e-03\n",
      "    2.06151952e-03   2.69595451e-03   1.22746648e-03   2.51122798e-03\n",
      "    1.81103464e-03   5.36202244e-04   1.53286190e-03   1.36069850e-03\n",
      "    8.41526033e-04   4.26013234e-04   6.06029852e-04   1.10886733e-03\n",
      "    6.62569411e-04   3.35888966e-04   7.23288147e-04   8.74564313e-04\n",
      "    1.51132658e-03   7.71399696e-04   4.74326325e-04   2.05480028e-03\n",
      "    4.88246119e-04   2.37769689e-04   2.09361958e-03   7.71120008e-04\n",
      "    1.40493549e-03   2.05488160e-03   4.51269534e-04   8.02277264e-04\n",
      "    6.02878563e-04   2.39844334e-03   1.44936435e-03   1.67907872e-03\n",
      "    1.91354549e-04   2.21182819e-03   1.49968745e-03   1.81117997e-04\n",
      "    1.56365221e-03   2.29238268e-03   1.32478244e-03   6.04184589e-04\n",
      "    2.09478081e-03   2.44257774e-03   2.40516988e-03   1.81307989e-03\n",
      "    1.09311271e-03   1.16504641e-05   1.49517012e-03   2.41268997e-03\n",
      "    1.06359599e-03   2.93985598e-05   2.47656944e-03   3.68364172e-04\n",
      "    2.33268903e-03   1.40760920e-03   1.66286522e-03   1.46403844e-03\n",
      "    3.15743937e-04   7.08275528e-04   1.63093654e-03   1.89687305e-03\n",
      "    1.71832427e-03   9.38032993e-04   8.30805911e-04   1.39731259e-03\n",
      "    7.25595560e-04   1.44968792e-03   5.22519884e-04   1.91268073e-03\n",
      "    1.83087812e-03   1.98917167e-05   1.12674061e-03   1.35956363e-03\n",
      "    1.39717296e-03   2.46023349e-04   1.51471930e-03   2.30147202e-04\n",
      "    1.48621108e-03   1.61129343e-03   5.21423915e-04   1.97209918e-03\n",
      "    1.24387566e-03   5.92157883e-04   1.90596023e-03   1.09689482e-03\n",
      "    1.92219758e-03   2.67011102e-03   2.58783315e-03   1.33137276e-03\n",
      "    9.30311898e-04   4.51281306e-04   2.48475322e-03   1.07982404e-03\n",
      "    1.09363589e-03   1.84633491e-03   2.00917536e-03   1.72183907e-03\n",
      "    4.52059350e-04   1.83364007e-03   1.34770733e-03   1.44924394e-03\n",
      "    1.67660444e-03   6.61061346e-04   6.65737746e-04   2.05370292e-03\n",
      "    2.55215878e-03   1.03846193e-03   3.93962351e-04   1.41418462e-03\n",
      "    1.74549962e-03   3.63537364e-04   1.76131012e-03   2.40609583e-03\n",
      "    1.06292741e-03   8.71879555e-04   2.29884367e-03   2.36213762e-03\n",
      "    2.36824539e-03   3.39294693e-04   1.65123802e-03   1.80936766e-03\n",
      "    2.39662699e-03   1.50452944e-03   1.92507276e-03   2.30508011e-04\n",
      "    9.08554172e-04   6.80500624e-04   9.49493137e-04   4.05758423e-04\n",
      "    2.40619886e-04   2.05837345e-03   5.06273344e-05   1.85228091e-04\n",
      "    7.03192730e-04   2.54245061e-03   1.19213140e-03   2.08477705e-05\n",
      "    9.00443578e-05   1.54689908e-03   1.16544947e-03   5.00927229e-04\n",
      "    1.38876607e-03   2.61319096e-03   1.68348737e-03   1.08811967e-03\n",
      "    2.22006936e-03   7.20015717e-04   2.70231915e-03   1.36721324e-03\n",
      "    5.25171494e-04   3.96623890e-04   1.27086294e-05   1.39642127e-03\n",
      "    1.23686755e-03   2.01184003e-03   1.02073162e-03   1.16454588e-05\n",
      "    1.42399467e-03   1.05992289e-03   2.46313628e-03   2.65649464e-03\n",
      "    2.38024382e-04   1.82198511e-03   1.82719130e-03   1.85303801e-03\n",
      "    2.87024664e-04   2.44053847e-03   1.25332657e-03   1.03104586e-03\n",
      "    4.26099405e-04   3.64185548e-04   8.97382008e-04   2.59316699e-03\n",
      "    7.45721252e-05   1.72168635e-03   1.11945830e-03   2.55729969e-03\n",
      "    1.59745806e-03   2.13868329e-03   4.64045707e-04   1.46045011e-03\n",
      "    2.69820160e-03   3.20639932e-04   2.96519079e-05   2.42315344e-04\n",
      "    1.80842179e-03   2.14651442e-03   7.84159043e-04   1.05644035e-03\n",
      "    9.05554773e-04   1.17136109e-03   1.81371745e-03   2.03904750e-03\n",
      "    2.06262111e-03   1.68399979e-03   1.31283588e-04   9.44425784e-04\n",
      "    2.39299486e-03   5.91128187e-04   2.62558458e-03   1.01031285e-03\n",
      "    1.53309613e-03   2.50973096e-03   1.33275110e-03   3.47753078e-04\n",
      "    2.54665159e-03   2.35954034e-03   1.33299736e-04   1.69175942e-03\n",
      "    1.78423127e-03   3.21900698e-04   3.93349994e-05   2.39014178e-03\n",
      "    1.44080174e-03   5.53030558e-04   5.25630346e-04   1.49784943e-03\n",
      "    1.62373469e-03   9.81249862e-04   4.39528872e-05   3.93550281e-04\n",
      "    2.44335287e-03   2.51035386e-03   1.71244946e-03   2.48853146e-04\n",
      "    1.71196890e-03   1.22515271e-03   8.02905687e-04   2.25004800e-03\n",
      "    1.19898382e-03   1.89483565e-03   2.65669775e-03   1.53927479e-03\n",
      "    1.23255664e-03   7.39632447e-04   2.51805303e-03   1.19693760e-03\n",
      "    6.80420369e-04   6.45523629e-04   1.61155705e-03   1.84231114e-03\n",
      "    1.43035353e-03   1.58886559e-03   1.28809962e-03   2.56598432e-03\n",
      "    5.72128498e-04   6.26698580e-04   1.18026614e-03   2.21044822e-03\n",
      "    1.41147531e-03   1.78084775e-03   1.59770371e-03   2.25083816e-03\n",
      "    1.49218647e-03   1.10279675e-03   1.36829049e-03   1.11944460e-03\n",
      "    2.21321509e-03   4.80727458e-04   2.46454387e-03   9.30796168e-04\n",
      "    2.39566516e-03   2.60134530e-03   5.24349573e-05   2.09285539e-03\n",
      "    1.57082260e-04   9.16595903e-04   1.47259953e-03   7.40785827e-04\n",
      "    1.46180006e-04   2.00673303e-03   2.37188716e-03   2.56436609e-03\n",
      "    1.15478912e-03   1.02279826e-03   6.20250743e-04   1.66194255e-03\n",
      "    5.23733507e-04   1.76854080e-03   1.12173511e-03   7.21726074e-04\n",
      "    1.29145892e-03   5.20358870e-04   1.23508688e-03   8.63818620e-04\n",
      "    2.62293898e-03   2.39104015e-03   2.46375631e-03   1.77542099e-03\n",
      "    1.87574871e-03   1.84949716e-03   1.79764675e-03   2.32562973e-03\n",
      "    2.37891375e-03   8.55941353e-04   1.67638409e-03   3.28298034e-04\n",
      "    6.57510049e-04   6.06195919e-04   4.12295884e-04   2.24493282e-03\n",
      "    2.84766115e-04   2.52952596e-03   2.93481792e-04   1.46882871e-03\n",
      "    2.04570465e-03   1.79590289e-04   8.24468805e-04   7.67277025e-04\n",
      "    2.67290914e-03   2.70617824e-03   2.51841224e-03   1.73548811e-03\n",
      "    1.14992709e-03   2.03131701e-03   1.63852516e-03   2.03888688e-03\n",
      "    1.70947202e-03   6.86230019e-04   1.66449085e-03   2.29264884e-03\n",
      "    2.08116728e-03   9.34773568e-04   2.18164214e-04   2.17145690e-03\n",
      "    1.21618420e-03   8.93778592e-04   2.30238483e-03   2.58799626e-03\n",
      "    2.39858521e-03   8.90444202e-04   1.35764098e-03   1.97176172e-03\n",
      "    7.85953288e-04   2.61806614e-03   2.14219411e-03   3.48686243e-04\n",
      "    1.55417319e-03   2.00522001e-03   1.06667116e-03   1.88247917e-03\n",
      "    1.71786624e-03   1.64921876e-03   4.95356140e-04   3.74791000e-04\n",
      "    2.44272894e-03   1.09010378e-03   5.58804127e-04   2.92153414e-04\n",
      "    2.66202486e-03   2.23571510e-03   1.46532483e-03   1.25775794e-03\n",
      "    1.31973789e-04   2.63547324e-03   2.70061051e-03   1.49657176e-03\n",
      "    2.41076235e-03   2.37374846e-03   2.00710110e-03   1.00001696e-03\n",
      "    9.30082155e-04   1.54148883e-03   2.54999319e-03   2.56264364e-03\n",
      "    1.76533425e-03   2.24735360e-03   3.15980511e-04   9.46170036e-04\n",
      "    6.81695549e-05   2.41226005e-03   7.18756346e-04   1.33817197e-03\n",
      "    5.44156088e-04   1.22482864e-03   1.03431452e-03   2.32478182e-03\n",
      "    1.89156688e-03   1.30332354e-03   2.49399867e-03   2.54255810e-03\n",
      "    9.83143129e-04   9.84204620e-05   1.01616919e-03   8.77085407e-04\n",
      "    1.12825939e-03   2.61062305e-03   1.74296022e-03   2.10895294e-03\n",
      "    2.31773924e-03   1.14267858e-03   1.00969656e-03   2.31190020e-03\n",
      "    6.44657378e-04   2.16376778e-03   2.53773156e-03   2.51083319e-03\n",
      "    1.98314434e-04   2.17274491e-03   1.61468660e-03   9.40759246e-04\n",
      "    1.87046718e-03   1.10813080e-03   1.89116679e-03   1.22035414e-03\n",
      "    2.04595033e-03   2.70538455e-03   1.78174922e-03   1.44984881e-04\n",
      "    9.99766097e-05   2.19251429e-03   1.31316461e-03   1.54140958e-03\n",
      "    2.16225774e-03   1.16552096e-03   1.58571804e-03   2.14038159e-03\n",
      "    1.84900017e-03   6.37740527e-04   1.17971042e-04   1.60025968e-03\n",
      "    1.06156838e-03   2.66574554e-03   5.72061868e-04   5.26376919e-04\n",
      "    2.59355935e-03   1.31701582e-03   2.06842186e-03   6.37421926e-04\n",
      "    2.19506718e-03   1.98593569e-03   2.65147774e-03   2.55364308e-03\n",
      "    1.99764755e-03   4.64613353e-04   2.25251413e-03   9.43483781e-04\n",
      "    1.25048123e-03   7.34665373e-04   2.00482123e-03   8.90546049e-04\n",
      "    1.10199442e-03   2.07544904e-03   1.70648848e-04   1.88251933e-03\n",
      "    2.67767440e-04   2.17014979e-03   9.91204424e-04   8.76740843e-04\n",
      "    2.62884472e-03   1.55787251e-03   8.56138079e-04   3.12029619e-04\n",
      "    2.47126181e-03   1.75354419e-04   2.54824325e-03   2.38844785e-03\n",
      "    1.08317909e-03   9.49598564e-04   2.43266857e-03   2.54738430e-03\n",
      "    9.86200041e-04   1.76942497e-03   1.09445158e-03   4.98992536e-04\n",
      "    2.56032439e-03   1.05183236e-03   2.27305834e-03   2.22513723e-03\n",
      "    2.20155318e-03   2.65196099e-03   2.03796574e-04   2.49577012e-03\n",
      "    1.68773193e-04   7.84813889e-04   9.02695883e-04   2.14058647e-03\n",
      "    1.04932191e-03   1.78625926e-03   2.02358232e-03   2.37011948e-03\n",
      "    1.27944342e-03   2.42330169e-03   2.59621743e-04   2.33483200e-04\n",
      "    3.46877239e-04   3.49682436e-04   6.39630643e-04   2.00565103e-03\n",
      "    2.18273279e-03   1.79386983e-03   8.85223965e-04   2.42909797e-03\n",
      "    1.65818868e-03   8.80027929e-04   6.81063965e-04   1.26652092e-03\n",
      "    2.01040618e-03   9.69159251e-04   3.15874059e-04   2.49438347e-03\n",
      "    1.82110654e-03   2.58213522e-03   1.18171639e-03   1.65123357e-03\n",
      "    5.45641916e-04   1.32451974e-04   1.34274973e-04   1.47190367e-03\n",
      "    3.18426296e-04   8.00614035e-04   1.36385627e-03   8.62814305e-05\n",
      "    2.02463064e-04   6.19162281e-04   2.10722671e-03   1.57316616e-03\n",
      "    1.84881134e-03   1.92786542e-03   2.69396819e-03   2.58073203e-03\n",
      "    5.93094199e-04   4.35808972e-04   3.11300618e-04   6.06830632e-04\n",
      "    6.44642350e-04   4.35377770e-04   1.87271452e-03   1.40201578e-04\n",
      "    2.48363774e-03   1.99052430e-03   7.54794104e-04   7.59117330e-04\n",
      "    1.50362602e-03   1.90889736e-03   2.04693538e-04   1.23026870e-03\n",
      "    2.46629560e-03   1.87422487e-03   5.44281693e-04   1.59666380e-03\n",
      "    2.20494595e-03   2.55936825e-04   1.89086119e-03   1.48458756e-03\n",
      "    2.40024221e-03   1.88824433e-03   1.33862335e-04   2.06111371e-03\n",
      "    1.74000001e-03   7.03176209e-04   2.77241808e-04   2.03508599e-03\n",
      "    1.76133650e-03   7.28202635e-04   1.59492089e-03   1.92884823e-03\n",
      "    1.54698960e-03   1.68798496e-03   1.02986484e-03   2.21228653e-03\n",
      "    1.98788608e-03   1.27920858e-03   2.15471466e-03   2.22030871e-03\n",
      "    2.21181782e-03   7.42841298e-05   2.23221281e-03   4.32923976e-04\n",
      "    1.02288970e-03   1.88621289e-03   2.06647712e-03   8.50483613e-04\n",
      "    1.23630803e-03   1.93371571e-03   1.02176158e-03   2.23258078e-03\n",
      "    2.29079039e-03   2.19849592e-03   9.07425969e-05   2.33719125e-03\n",
      "    2.13906770e-03   1.77369764e-03   2.43993167e-03   2.04074551e-03\n",
      "    6.20846594e-04   8.23463969e-04   2.40262421e-03   2.55463946e-03\n",
      "    7.22194351e-04   4.97959935e-04   2.06593847e-03   1.59369841e-03\n",
      "    2.04359237e-04   1.04515827e-03   2.37537663e-04   2.42948082e-03\n",
      "    8.12386004e-04   1.63848867e-03   2.12258053e-03   5.66857879e-04\n",
      "    7.41090478e-04   1.39474192e-03   1.52280375e-04   8.20655701e-04\n",
      "    1.26180183e-03   1.35566716e-03   1.92112239e-04   2.09604419e-03\n",
      "    2.33483955e-03   2.68092205e-03   2.44128794e-03   2.37897242e-03\n",
      "    1.83512707e-03   1.63991715e-03   6.32645572e-04   2.04238902e-03\n",
      "    8.72887183e-04   2.51992373e-04   1.36587621e-03   1.31299374e-03\n",
      "    1.86353812e-03   2.18773744e-04   9.91275936e-04   1.12444459e-04\n",
      "    2.51717754e-03   2.40357385e-04   1.38354858e-03   9.42224525e-04\n",
      "    2.09499034e-03   6.57565433e-04   1.27017207e-03   6.29668980e-04\n",
      "    8.29224150e-04   1.51803475e-03   2.67402687e-04   8.76280449e-04\n",
      "    8.63994286e-04   2.44839951e-03   1.38647489e-03   2.84089408e-04\n",
      "    1.16696310e-03   1.16347810e-03   1.94414860e-03   1.40538451e-04\n",
      "    6.20315961e-04   2.01477700e-03   2.58303899e-03   1.86850174e-03\n",
      "    1.87573542e-03   2.05844276e-03   3.23267031e-04   2.51679348e-04\n",
      "    1.65943569e-04   9.81723835e-04   2.08080211e-03   2.04380858e-03\n",
      "    2.13954332e-03   1.91523899e-03   2.60527036e-03   2.17061296e-03\n",
      "    2.61156430e-03   7.00461144e-04   1.09564775e-04   1.16754904e-03\n",
      "    2.14275184e-03]]\n",
      "1.0\n",
      "[147]\n"
     ]
    }
   ],
   "source": [
    "def sample_embedding(prediction): #parece que distibution esta en prediction[0], es la primera columna\n",
    "  \"\"\"Turn a (column) prediction into embed sample.\"\"\"\n",
    "  p = np.zeros(shape=[1,], dtype=np.int) \n",
    "  p[0] = sample_distribution(prediction[0])\n",
    "  return p #p.shape=[1,]\n",
    "\n",
    "def bigram_random_distribution(): #las normaliza en el return\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, embedding_size])\n",
    "  return b/np.sum(b, 1)[:,None] #shape=(1,27)\n",
    "\n",
    "#DAF understanding\n",
    "d= bigram_random_distribution()\n",
    "print(d)\n",
    "print(sum(d[0]))\n",
    "print(sample_embedding(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = vocabulary_size * vocabulary_size\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    " \n",
    "\n",
    "  # Parameters:\n",
    "  # All the gates: input, previous output, and bias.\n",
    "  cix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1)) # cix.shape=[729, 256]\n",
    "  cim = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1)) # cim.shape=[64, 256]\n",
    "  cib = tf.Variable(tf.zeros([1, num_nodes * 4])) # cib.shape=[1,256]\n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) # saved_output.shape=[64,64]\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) # saved_state.shape=[64,64]\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, embedding_size], -0.1, 0.1)) # w.shape=[64,729]\n",
    "  b = tf.Variable(tf.zeros([embedding_size])) # b.shape =[729,]\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, train=False): #i:input shape=[64,27], o:saved_output (shape=[64,64]), state:saved_state (shape=[64,64])\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    embed = tf.nn.embedding_lookup(cix ,i)   \n",
    "    if train:\n",
    "        embed = tf.nn.dropout(embed, 0.5)\n",
    "    \n",
    "    all_gates = embed + tf.matmul(o, cim) + cib # all_gates.shape=[64, 256]\n",
    "    #  embedding_lookup(cix.shape=[729, 256], i.shape=[64,]) --> embedding_lookup.shape=[64, 256]\n",
    "    #+ matmul(o.shape=[64,64], cim.shape=[64, 256]) --> matmul.shape=[64, 256], +.shape=[64, 256]\n",
    "    #+ cib.shape=[1, 256] -->  all_gates.shape=[64, 256] \n",
    "       \n",
    "    input_gate = tf.sigmoid(all_gates[:, 0:num_nodes])\n",
    "    \n",
    "    forget_gate = tf.sigmoid(all_gates[:, num_nodes:2*num_nodes])\n",
    "    \n",
    "    update = all_gates[:, 2*num_nodes:3*num_nodes]\n",
    "    \n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    \n",
    "    output_gate = tf.sigmoid(all_gates[:, 3*num_nodes:])\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append( \n",
    "      tf.placeholder(tf.int32, shape=[batch_size])) #<--- OJO: batch.shape=[64,] -> 64 embeddings = 64 ids ,\n",
    "\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output # shape=[64,64]\n",
    "  state = saved_state # shape=[64,64]\n",
    "    \n",
    "  for i in train_inputs: # 10 x shape=[64,]\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "\n",
    "    # Classifier.\n",
    "    \n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # shape=[640, 729]\n",
    "       \n",
    "    loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=tf.one_hot(tf.concat(train_labels,0), embedding_size)))    \n",
    "    #labels.shape=[640, 729]\n",
    "    \n",
    "    #loss = tf.reduce_mean(\n",
    "      #tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        #logits,  tf.concat(0, train_labels)))\n",
    "        \n",
    "    #DAF understanding\n",
    "    #print(\"logits: \", logits)\n",
    "    #print(\"labels:\", tf.one_hot(tf.concat(train_labels,0), embedding_size))\n",
    "    #print(\"softmax_cross_entropy_with_logits:\", tf.nn.softmax_cross_entropy_with_logits(\n",
    "            #logits=logits, labels=tf.one_hot(tf.concat(train_labels,0), embedding_size)))\n",
    "        \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    \n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss)) \n",
    "  #This is the first part of minimize(). \n",
    "  #It returns a list of (gradient, variable) pairs where \"gradient\" is the gradient for \"variable\". \n",
    "\n",
    "  #DAF: entremedias se hace el \"Gradient Clipping\"  \n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step) \n",
    "  #This is the second part of minimize(). It returns an Operation that applies gradients.\n",
    "  #grads_and_vars: List of (gradient, variable) pairs as returned by compute_gradients()\n",
    "\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits) # shape=[640, 729]\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1]) #un embeding shape=[1]\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes])) #shape=[1,64]\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes])) #shape=[1,64]\n",
    "  \n",
    "  reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), \n",
    "                                saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "\n",
    "\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):        \n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b)) #1 bigram 1hot encoding: shape=[1,729]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.590238 learning rate: 10.000000\n",
      "Minibatch perplexity: 727.95\n",
      "================================================================================\n",
      "(nn)(ew)(yq)(vz)(tc)(vy)(wv)( n)(cp)(dn)(x )(th)(th)(kg)(qn)(dq)(yz)(lf)(jp)(pj)(c )(kk)(av)(if)(vp)(bk)(ov)(it)(ti)(ef)(am)(ok)(th)(kz)(va)(np)(py)(fu)(pp)(rt)(qm)(fd)(pn)(hd)(xj)(nc)(ib)(mb)(zl)(hz)(ie)(qu)(bn)(sa)(bz)(wg)(gw)(c )(wp)(no)(mj)(ng)(uc)(vr)(mi)(yd)(ae)(tf)(op)(lf)(yt)(vl)(io)(oq)(ss)(jd)(ii)(lo)(ma)(al)\n",
      "(gt)(ze)(  )(ps)(wf)(uv)(xq)(mf)(vy)(ah)(ri)(ti)(bm)(rt)(lh)(lh)(ue)(zg)(bo)(io)(ql)(tk)(bk)(av)(j )(qi)(yb)(yb)(zv)(td)(jw)( z)(qx)(lv)(oq)(hy)(li)(u )(oz)(n )(ku)(lx)(ef)(ab)(uv)(nc)(tf)(gs)(lt)(ha)(wc)(nb)(wj)(iu)(ew)(tv)(lu)(oi)(je)(zh)(tu)(l )(yg)(tp)(lv)(jf)(oz)(cf)(ey)(uo)(rq)(ru)(ca)(ib)(bw)(kq)(f )(mh)(tr)(no)\n",
      "(rn)(mb)(pd)(dy)(an)(hz)(gf)(lp)(ex)(os)(zn)( l)(ic)(xu)(aj)(dl)(ag)(xf)(cs)(uy)(qv)(cq)(oq)(ll)(cw)(dv)(nh)(kh)(kz)(te)(ht)(vp)(cn)(go)(bg)(pn)(us)(nq)(rl)(dy)( b)(qm)(lk)(kf)(ph)(ff)(au)(vt)(lr)(hu)(ks)(kp)(tu)(of)(mm)(qb)(tk)(ap)(xf)(wp)(pg)(wd)(nl)(sd)(nq)(zm)(kl)(ez)(za)(u )(ot)(at)(oj)(ck)(c )(oo)(ot)(sb)(lp)(au)\n",
      "(zq)(as)(tg)(wr)(lx)(hf)(hj)(ef)(gs)(ty)(nt)(pj)(dw)(cq)(vh)(cb)(jy)(im)(ta)(vy)(gk)(xb)(id)(oj)(ez)(ct)(lq)(qg)(vu)(wb)(li)(gn)(cr)(tr)(ih)(ij)(du)(os)(ch)(jf)(ej)(hi)(ug)(lo)(ba)(gz)(ay)(sl)(hs)(mb)(sz)(no)(nm)(ox)(ho)(j )(qe)(ol)(gx)(pc)(fa)(vd)(cs)(xx)(ju)(sq)(oz)(tn)(zk)(gn)(oz)(lt)(xy)(ne)(hb)(zn)(jn)(as)(xp)(wp)\n",
      "(i )(pa)(mq)(jr)(vs)(qp)( v)(xn)(dh)(zd)( d)(kg)(yi)(nv)(yr)(zr)(mr)(qd)(eh)(ty)(zz)(w )(in)(vi)(hw)(ch)(vs)(py)(nr)(rg)(qq)(au)(hp)(ox)(ac)(xy)(bq)(wq)(gb)(vs)(zv)(sr)(kb)(gv)(ox)(ll)(hw)(jh)(cy)(tr)( h)( s)(ba)(cj)(xv)(gl)(kw)(bh)(im)( r)(oc)(ca)(zb)(ju)(de)(io)(mc)(fh)(wa)(ah)(p )(lm)(xw)(mt)(hp)(at)(df)(ri)(vw)(lw)\n",
      "================================================================================\n",
      "Validation set perplexity: 675.23\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-1ac930be63ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m       \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/david/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/david/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/david/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/david/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/david/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#num_steps = 7001\n",
    "num_steps = 1101\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "\n",
    "  for step in range(num_steps):\n",
    "\n",
    "    batches = bigram_train_batches.next()\n",
    "    feed_dict = dict()   \n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i] \n",
    "    \n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    \n",
    "    if step % summary_frequency == 0:\n",
    "        \n",
    "      if step > 0: \n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "    \n",
    "      labels = np.concatenate(list(batches)[1:]) #labels.shape=[640,]\n",
    "      # convert to one-hot-encodings\n",
    "      noembed_labels = np.zeros(predictions.shape) #predictions.shape=[640, 729]\n",
    "      for i, j in enumerate(labels): #DAF i=0,1,2, j=el valor\n",
    "        noembed_labels[i, j] = 1.0\n",
    "\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, noembed_labels))))\n",
    "    \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample_embedding(bigram_random_distribution()) #feed es una bigram sampleado [1,729]. El primero de cada seq al azar\n",
    "          sentence = characters_from_embed(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          \n",
    "          for _ in range(79):\n",
    "            #DAF feed alimenta, via la var sample input, el proceso de sample_prediction\n",
    "            #y luego se vuelve a samplear un nuevo feed (una nueva letra), basado en la prediccion de sample_prediction \n",
    "            prediction = sample_prediction.eval({sample_input: feed}) \n",
    "            feed = sample_embedding(prediction)\n",
    "            sentence += characters_from_embed(feed)[0]\n",
    "            \n",
    "          print(sentence)  \n",
    "        print('=' * 80)\n",
    "        \n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "    \n",
    "      for _ in range(valid_size): #DAF: valid_size = 1000, valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "        b = bigram_valid_batches.next() #b.shape=[1,1+1=2]\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]}) #se hace la prediccion con el primer bigram\n",
    "        labels = np.zeros((1, bigram_size))\n",
    "        labels[0, b[1]] = 1.0\n",
    "        valid_logprob = valid_logprob + logprob(predictions, labels) # la label es el segundo bigram\n",
    "  \n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda2/lib/python2.7/site-packages/tensorflow\n",
      "['', '/home/david/anaconda2/lib/python27.zip', '/home/david/anaconda2/lib/python2.7', '/home/david/anaconda2/lib/python2.7/plat-linux2', '/home/david/anaconda2/lib/python2.7/lib-tk', '/home/david/anaconda2/lib/python2.7/lib-old', '/home/david/anaconda2/lib/python2.7/lib-dynload', '/home/david/anaconda2/lib/python2.7/site-packages/Sphinx-1.3.5-py2.7.egg', '/home/david/anaconda2/lib/python2.7/site-packages/setuptools-20.3-py2.7.egg', '/home/david/anaconda2/lib/python2.7/site-packages', '/home/david/anaconda2/lib/python2.7/site-packages/IPython/extensions', '/home/david/.ipython', '/home/david/anaconda2/lib/python2.7/site-packages/tensorflow/contrib']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(tf.__path__[0]+'/contrib')\n",
    "print(tf.__path__[0])\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DAF: hemos copiado seq2seq_model.py y data_utils.py en el directorio de trabajo desde /home/david/coursera/models/tutorials/rnn/translate\n",
    "import seq2seq_model as seq2seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "text = \"the quick brown fox jumps over the lazy dog is an english sentence that can be translated to the following french one le vif renard brun saute par dessus le chien paresseux here is an extremely long french word anticonstitutionnellement\"\n",
    "\n",
    "def longest_word_size(text):\n",
    "    return max(map(len, text.split()))\n",
    "\n",
    "word_size = longest_word_size(text)\n",
    "print(word_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "num_nodes = 64\n",
    "batch_size = 10\n",
    "\n",
    "def create_model():\n",
    "     return seq2seq_model.Seq2SeqModel(source_vocab_size=vocabulary_size, #27 letras\n",
    "                                   target_vocab_size=vocabulary_size, #27 letras\n",
    "                                   buckets=[(word_size + 1, word_size + 2)], # only 1 bucket #DAF: buckets=[(25+1,25+2)] \n",
    "                                   size=num_nodes, # 64 nodos\n",
    "                                   num_layers=3, #valor heredado de tutorial\n",
    "                                   max_gradient_norm=5.0, #valor heredado de tutorial\n",
    "                                   batch_size=batch_size, # 10 items\n",
    "                                   learning_rate=0.5, #valor heredado de tutorial\n",
    "                                   learning_rate_decay_factor=0.99, #valor heredado de tutorial\n",
    "                                   use_lstm=True,\n",
    "                                   forward_only=False) #valor heredado de tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    encoder_inputs = [np.random.randint(1, vocabulary_size, word_size + 1) for _ in xrange(batch_size)]\n",
    "    decoder_inputs = [np.zeros(word_size + 2, dtype=np.int32) for _ in xrange(batch_size)]\n",
    "    weights = [np.ones(word_size + 2, dtype=np.float32) for _ in xrange(batch_size)]\n",
    "    for i in xrange(batch_size):\n",
    "        r = random.randint(1, word_size)\n",
    "        # leave at least a 0 at the end\n",
    "        encoder_inputs[i][r:] = 0\n",
    "        # one 0 at the beginning of the reversed word, one 0 at the end\n",
    "        decoder_inputs[i][1:r+1] = encoder_inputs[i][:r][::-1]\n",
    "        weights[i][r+1:] = 0.0\n",
    "    return np.transpose(encoder_inputs), np.transpose(decoder_inputs), np.transpose(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EJM DE UNA FILA (1/10) con r= 14 :\n",
      "\n",
      "Un encoder_input:  [ 3  3  8 21 21 15 24  5 22 25 12 19 11 25  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0]\n",
      "Su decoder_inputs:  [ 0 25 11 19 12 25 22  5 24 15 21 21  8  3  3  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n",
      "Los weights para ellos:  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "Las 3 estructuras se devuelven Transposed. 10 FILAS se convierten en 10 COLUMNAS:\n",
      "\n",
      "np.transpose(encoder_inputs):  (26, 10) \n",
      " [[ 3  7 21 19 11  8  3  4 10 26]\n",
      " [ 3 16 17  0  0 25 11 12 25 20]\n",
      " [ 8 19 20  0  0 24 26 25  7 24]\n",
      " [21 22  0  0  0 19 21  7 15 11]\n",
      " [21  2  0  0  0 21 14  4  9 14]\n",
      " [15  0  0  0  0 23 18 20 11 22]\n",
      " [24  0  0  0  0 15 23  7 10 19]\n",
      " [ 5  0  0  0  0 10 21  0 24 12]\n",
      " [22  0  0  0  0 22 21  0  5  7]\n",
      " [25  0  0  0  0  3 21  0 12  2]\n",
      " [12  0  0  0  0  8 10  0 14  2]\n",
      " [19  0  0  0  0  2  2  0  7 16]\n",
      " [11  0  0  0  0 11  4  0  8  3]\n",
      " [25  0  0  0  0  1 10  0 26 14]\n",
      " [ 0  0  0  0  0 22  6  0 24  6]\n",
      " [ 0  0  0  0  0  6  8  0  1 21]\n",
      " [ 0  0  0  0  0 24  0  0  9  0]\n",
      " [ 0  0  0  0  0 12  0  0 13  0]\n",
      " [ 0  0  0  0  0 13  0  0  0  0]\n",
      " [ 0  0  0  0  0  5  0  0  0  0]\n",
      " [ 0  0  0  0  0 12  0  0  0  0]\n",
      " [ 0  0  0  0  0 26  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "\n",
      "np.transpose(decoder_inputs):  (27, 10) \n",
      " [[ 0  0  0  0  0  0  0  0  0  0]\n",
      " [25  2 20 19 11 26  8  7 13 21]\n",
      " [11 22 17  0  0 12  6 20  9  6]\n",
      " [19 19 21  0  0  5 10  4  1 14]\n",
      " [12 16  0  0  0 13  4  7 24  3]\n",
      " [25  7  0  0  0 12  2 25 26 16]\n",
      " [22  0  0  0  0 24 10 12  8  2]\n",
      " [ 5  0  0  0  0  6 21  4  7  2]\n",
      " [24  0  0  0  0 22 21  0 14  7]\n",
      " [15  0  0  0  0  1 21  0 12 12]\n",
      " [21  0  0  0  0 11 23  0  5 19]\n",
      " [21  0  0  0  0  2 18  0 24 22]\n",
      " [ 8  0  0  0  0  8 14  0 10 14]\n",
      " [ 3  0  0  0  0  3 21  0 11 11]\n",
      " [ 3  0  0  0  0 22 26  0  9 24]\n",
      " [ 0  0  0  0  0 10 11  0 15 20]\n",
      " [ 0  0  0  0  0 15  3  0  7 26]\n",
      " [ 0  0  0  0  0 23  0  0 25  0]\n",
      " [ 0  0  0  0  0 21  0  0 10  0]\n",
      " [ 0  0  0  0  0 19  0  0  0  0]\n",
      " [ 0  0  0  0  0 24  0  0  0  0]\n",
      " [ 0  0  0  0  0 25  0  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "\n",
      "np.transpose(weights):  (27, 10) \n",
      " [[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  0.  0.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  0.  0.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  0.  0.  0.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  0.  0.  0.  1.  1.  1.  1.  1.]\n",
      " [ 1.  0.  0.  0.  0.  1.  1.  1.  1.  1.]\n",
      " [ 1.  0.  0.  0.  0.  1.  1.  1.  1.  1.]\n",
      " [ 1.  0.  0.  0.  0.  1.  1.  0.  1.  1.]\n",
      " [ 1.  0.  0.  0.  0.  1.  1.  0.  1.  1.]\n",
      " [ 1.  0.  0.  0.  0.  1.  1.  0.  1.  1.]\n",
      " [ 1.  0.  0.  0.  0.  1.  1.  0.  1.  1.]\n",
      " [ 1.  0.  0.  0.  0.  1.  1.  0.  1.  1.]\n",
      " [ 1.  0.  0.  0.  0.  1.  1.  0.  1.  1.]\n",
      " [ 1.  0.  0.  0.  0.  1.  1.  0.  1.  1.]\n",
      " [ 0.  0.  0.  0.  0.  1.  1.  0.  1.  1.]\n",
      " [ 0.  0.  0.  0.  0.  1.  1.  0.  1.  1.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#DAF understanding of the BATCH GENERATION at EACH STEP\n",
    "\n",
    "encoder_inputs = [np.random.randint(1, vocabulary_size, word_size + 1) for _ in xrange(batch_size)] #10 x [26,]\n",
    "decoder_inputs = [np.zeros(word_size + 2, dtype=np.int32) for _ in xrange(batch_size)] #10 x [27,]\n",
    "weights = [np.ones(word_size + 2, dtype=np.float32) for _ in xrange(batch_size)] #10 x [27,]\n",
    "#print(\"encoder_inputs: \", encoder_inputs[0])\n",
    "#print(\"decoder_inputs: \", decoder_inputs[0])\n",
    "#print(\"weights: \", weights[0])\n",
    "\n",
    "for i in xrange(batch_size):\n",
    "    r = random.randint(1, word_size)\n",
    "    if i==0: pos=r\n",
    "    # leave at least a 0 at the end\n",
    "    encoder_inputs[i][r:] = 0\n",
    "    # one 0 at the beginning of the reversed word, one 0 at the end\n",
    "    decoder_inputs[i][1:r+1] = encoder_inputs[i][:r][::-1]\n",
    "    weights[i][r+1:] = 0.0\n",
    "    \n",
    "print(\"EJM DE UNA FILA (1/10) con r=\", pos, \":\\n\")\n",
    "print(\"Un encoder_input: \", encoder_inputs[0]) #a partir de pos r rellena de 0s \n",
    "print(\"Su decoder_inputs: \", decoder_inputs[0]) #(previo lleno de 0s), a partir de pos 1, mete el input \"reversed\", queda 0 al final\n",
    "print(\"Los weights para ellos: \", weights[0]) #(previo lleno de 1s) a partir de res r+1 rellena con 0.0s. \n",
    "#Los weights quedan a 1 hasta la posicion en que termina de haber valores<>0 en (encoder_inputs) y decoder_inputs\n",
    "#: r+1. Luego todo 0s\n",
    "\n",
    "#Las 3 se devuelven tranposed. 10 FILAS se convierten en 10 COLUMNAS: CADA COLUMNA ES UN ITEM DEL BATCH\n",
    "print(\"\\nLas 3 estructuras se devuelven Transposed. 10 FILAS se convierten en 10 COLUMNAS:\\n\") \n",
    "print(\"np.transpose(encoder_inputs): \", np.transpose(encoder_inputs).shape, \"\\n\", np.transpose(encoder_inputs))\n",
    "print(\"\\nnp.transpose(decoder_inputs): \", np.transpose(decoder_inputs).shape, \"\\n\", np.transpose(decoder_inputs))\n",
    "print(\"\\nnp.transpose(weights): \", np.transpose(weights).shape, \"\\n\", np.transpose(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strip_zeros(word):\n",
    "    # 0 is the code for space in char2id()\n",
    "    return word.strip(' ') #quita los espacios al principio y final de word\n",
    "\n",
    "def evaluate_model(model, sess, words, encoder_inputs):\n",
    "        \n",
    "    correct = 0\n",
    "    \n",
    "    #DAF: CLAVE: MIENTRAS EN EL TRAINING decoder_inputs y weigths se pasan a model.step() con valores \"coherentes\" \n",
    "    # generados por \"Ad-Hoc para el training\" por getBatch()) \n",
    "    # Aqui el model.step esta forward_only=True. O sea: se usa para ESTIMAR LOS OUTPUTS. Por eso AL INICIO DEL BUCLE \n",
    "    # decoder_inputs y target_weights se pasan model.step VACIOS (a cero). Luego se van actualizando a cada step del bucle\n",
    "    # en funcion de los OUTPUTS (output_logits) y volviendose a pasar (ya con contenido)\n",
    "    \n",
    "    # Finalmente despues del bucle, el decoder_inputs resultante producir la EVALUACION: comparandolo con las palabras\n",
    "    # originales del batch (range_words)\n",
    "    \n",
    "    decoder_inputs = np.zeros((word_size + 2, batch_size), dtype=np.int32) #shape=[27,10], de 0s\n",
    "    target_weights = np.zeros((word_size + 2, batch_size), dtype=np.float32) #shape=[27,10], de 0.0s\n",
    "    target_weights[0,:] = 1.0 #target_weights[0] se pone todo a 1.0s\n",
    "    \n",
    "    is_finished = np.full(batch_size, False, dtype=np.bool_) #[False False False False False False False False False False]\n",
    "    \n",
    "    for i in xrange(word_size + 1): #DAF: caracter a carcacter de la WORD=25+1=26 (nada que ver con vocabulary_size=27)\n",
    "        \n",
    "        _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id=0, forward_only=True)\n",
    "               \n",
    "        # DAF: output_logits[0] is the predicted probability distribution for the first character. (EN LAS 10 PALABRAS)\n",
    "        # I then run step() again in forward-only mode to get the predicted probability distribution for \n",
    "        # the second character, etc, etc.\n",
    "        # output_logits[0].shape : (10, 27) *****este 27 es por vocabulary_size****, NO por (max_)word_size \n",
    "        # Y len(output_logits) = 27. Puesto que es una python list es xrange(word_size+1) = 0-26 = 27 \n",
    "        # ***Este 27 es distinto al otro. Es por el tamaño word_size=25\n",
    "        \n",
    "        p = np.argmax(output_logits[i], axis=1) \n",
    "        \n",
    "        #DAF: para este los caracteres ith de encoder_input tenemos su output_logits[i]\n",
    "        #output_logits[i].shape : (filas=10=words, columnas=27=chars)\n",
    "        #p son los 10 indices de la columna (de 0 a 26)(por axis=1) que tiene el mayor valor\n",
    "        #print(p)\n",
    "        \n",
    "        #DAF: Actualizacion de decoder_inputs y target_weights en funcion de p (que es un resumen de output_logits[i])\n",
    "        # de (el estado) de is_finished. #TODO: ENTENDER EL DETALLE DE ESTA ACTUALIZACION\n",
    "        \n",
    "        is_finished = np.logical_or(is_finished, p == 0) \n",
    "        \n",
    "        decoder_inputs[i,:] = (1 - is_finished) * p \n",
    "        \n",
    "        target_weights[i,:] = (1.0 - is_finished) * 1.0 \n",
    "        \n",
    "        \n",
    "        #if np.all(is_finished):\n",
    "            #break\n",
    "\n",
    "    #DAF understanding\n",
    "    #print(\"encoder_inputs: \", encoder_inputs, encoder_inputs.shape)\n",
    "    print(\"decoder_inputs: \\n\",decoder_inputs, decoder_inputs.shape)    \n",
    "    #print(\"target_weights: \", target_weights, target_weights.shape)\n",
    "    #print(\"len(output_logits) :\",len(output_logits)) # len(output_logits) =27\n",
    "    #print(\"output_logits[0].shape :\",output_logits[0].shape)\n",
    "    #print(\"output_logits[0]:\",output_logits[0]) #salida superverbosa\n",
    "    print(\"P es \", np.argmax(output_logits[0], axis=1), np.argmax(output_logits[0], axis=1).shape,\"\\n\")\n",
    "    \n",
    "    #DAF: EVALUACION: comparacion directa de output_word (generada a partir de decoder_inputs) O SEA LA PALABRA ESTIMADA, \n",
    "    #contra la reversed_word (inversa de la word original del batch) O SEA LA LABEL\n",
    "    #si son iguales -> correct++\n",
    "    \n",
    "    for idx, l in enumerate(np.transpose(decoder_inputs)):\n",
    "        \n",
    "        reversed_word = ''.join(reversed(words[idx]))\n",
    "        \n",
    "        output_word = strip_zeros(''.join(id2char(i) for i in l))\n",
    "        \n",
    "        print(words[idx], '(reversed: {0})'.format(reversed_word),\n",
    "              '-> [', output_word, '] ({0})'.format('OK' if reversed_word == output_word else 'KO'))\n",
    "        \n",
    "        \n",
    "        if reversed_word == output_word:\n",
    "            correct += 1\n",
    "    \n",
    "    \n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_batch(words):\n",
    "    encoder_inputs = [np.zeros(word_size + 1, dtype=np.int32) for _ in xrange(batch_size)]\n",
    "    for i, word in enumerate(words):\n",
    "        for j, c in enumerate(word):\n",
    "            encoder_inputs[i][j] = char2id(c)\n",
    "    return np.transpose(encoder_inputs)\n",
    "\n",
    "#DAF: words=range_words[i]=10 words -> \n",
    "#encoder_inputs=lista de 10 items (1 por word). Tamaño=26 (palabra más larga+1) \n",
    "#Desde pos 0, los ids de las letras, el resto son 0s. Se devuelve transposed: shape = 26 filas por 10 columnas\n",
    "\n",
    "def validate_model(text, model, sess):\n",
    "    words = text.split()\n",
    "    nb_words = (len(words) / batch_size) * batch_size\n",
    "    \n",
    "    correct = 0\n",
    "    for i in xrange(nb_words / batch_size):\n",
    "        range_words = words[i * batch_size:(i + 1) * batch_size]\n",
    "        \n",
    "        encoder_inputs = get_validation_batch(range_words)\n",
    "        \n",
    "        correct += evaluate_model(model, sess, range_words, encoder_inputs)\n",
    "    \n",
    "    print('* correct: {0}/{1} -> {2}%'.format(correct, nb_words, (float(correct) / nb_words) * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words):  40\n",
      "nb_words:  40\n",
      "nb_words / batch_size:  4\n",
      "\n",
      " 0  range_words:  ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'is'] 10\n",
      "encoder_inputs:  [[20 17  2  6 10 15 20 12  4  9]\n",
      " [ 8 21 18 15 21 22  8  1 15 19]\n",
      " [ 5  9 15 24 13  5  5 26  7  0]\n",
      " [ 0  3 23  0 16 18  0 25  0  0]\n",
      " [ 0 11 14  0 19  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]] (26, 10)\n",
      "\n",
      " 1  range_words:  ['an', 'english', 'sentence', 'that', 'can', 'be', 'translated', 'to', 'the', 'following'] 10\n",
      "encoder_inputs:  [[ 1  5 19 20  3  2 20 20 20  6]\n",
      " [14 14  5  8  1  5 18 15  8 15]\n",
      " [ 0  7 14  1 14  0  1  0  5 12]\n",
      " [ 0 12 20 20  0  0 14  0  0 12]\n",
      " [ 0  9  5  0  0  0 19  0  0 15]\n",
      " [ 0 19 14  0  0  0 12  0  0 23]\n",
      " [ 0  8  3  0  0  0  1  0  0  9]\n",
      " [ 0  0  5  0  0  0 20  0  0 14]\n",
      " [ 0  0  0  0  0  0  5  0  0  7]\n",
      " [ 0  0  0  0  0  0  4  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]] (26, 10)\n",
      "\n",
      " 2  range_words:  ['french', 'one', 'le', 'vif', 'renard', 'brun', 'saute', 'par', 'dessus', 'le'] 10\n",
      "encoder_inputs:  [[ 6 15 12 22 18  2 19 16  4 12]\n",
      " [18 14  5  9  5 18  1  1  5  5]\n",
      " [ 5  5  0  6 14 21 21 18 19  0]\n",
      " [14  0  0  0  1 14 20  0 19  0]\n",
      " [ 3  0  0  0 18  0  5  0 21  0]\n",
      " [ 8  0  0  0  4  0  0  0 19  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]] (26, 10)\n",
      "\n",
      " 3  range_words:  ['chien', 'paresseux', 'here', 'is', 'an', 'extremely', 'long', 'french', 'word', 'anticonstitutionnellement'] 10\n",
      "encoder_inputs:  [[ 3 16  8  9  1  5 12  6 23  1]\n",
      " [ 8  1  5 19 14 24 15 18 15 14]\n",
      " [ 9 18 18  0  0 20 14  5 18 20]\n",
      " [ 5  5  5  0  0 18  7 14  4  9]\n",
      " [14 19  0  0  0  5  0  3  0  3]\n",
      " [ 0 19  0  0  0 13  0  8  0 15]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0 21  0  0  0 12  0  0  0 19]\n",
      " [ 0 24  0  0  0 25  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 13]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]] (26, 10)\n"
     ]
    }
   ],
   "source": [
    "#DAF understanding\n",
    "words = text.split()\n",
    "nb_words = (len(words) / batch_size) * batch_size #b es por \"bucket\" \n",
    "print(\"len(words): \", len(words)) #40ytantos\n",
    "print(\"nb_words: \", nb_words) #40\n",
    "print(\"nb_words / batch_size: \", nb_words / batch_size) #4\n",
    "for i in xrange(nb_words / batch_size):\n",
    "        range_words = words[i * batch_size:(i + 1) * batch_size] #DAF range_words[i] = words[0:9],words[10:19],words[20:29]\n",
    "        encoder_inputs = get_validation_batch(range_words)\n",
    "        print(\"\\n\", i, \" range_words: \", range_words , len(range_words))\n",
    "        print(\"encoder_inputs: \", encoder_inputs, encoder_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverse_text(nb_steps):\n",
    "    with tf.Session() as session:\n",
    "        model = create_model()\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for step in xrange(nb_steps):\n",
    "            enc_inputs, dec_inputs, weights = get_batch()\n",
    "            _, loss, _ = model.step(session, enc_inputs, dec_inputs, weights, 0, False)\n",
    "            \n",
    "            if step % 1000 == 1:\n",
    "            #if step % 1 == 1:\n",
    "                print('* step:', step, 'loss:', loss)\n",
    "                validate_model(text, model, session)\n",
    "        \n",
    "        print('*** evaluation! loss:', loss)\n",
    "        validate_model(text, model, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* step: 1 loss: 3.28325\n",
      "=================\n",
      "DESPUES DEL BUCLE\n",
      "=================\n",
      "decoder_inputs:  [[12 12 12 12 12 12 12 12 12 12]\n",
      " [12 12 12 12 12 12 12 12 12 12]\n",
      " [12 12 12 12 12 12 12 12 12 12]\n",
      " [12 12 12 12 12 12 12 12 12 12]\n",
      " [ 0 12  0  0 12  0  0  0  0  0]\n",
      " [ 0  0  0  0 12  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]] (27, 10)\n",
      "P es  [12 12 12 12 12 12 12 12 12 12] (10,) \n",
      "\n",
      "the (reversed: eht) -> [ llll ] (KO)\n",
      "quick (reversed: kciuq) -> [ lllll ] (KO)\n",
      "brown (reversed: nworb) -> [ llll ] (KO)\n",
      "fox (reversed: xof) -> [ llll ] (KO)\n",
      "jumps (reversed: spmuj) -> [ llllll ] (KO)\n",
      "over (reversed: revo) -> [ llll ] (KO)\n",
      "the (reversed: eht) -> [ llll ] (KO)\n",
      "lazy (reversed: yzal) -> [ llll ] (KO)\n",
      "dog (reversed: god) -> [ llll ] (KO)\n",
      "is (reversed: si) -> [ llll ] (KO)\n",
      "=================\n",
      "DESPUES DEL BUCLE\n",
      "=================\n",
      "decoder_inputs:  [[12 12 12 12 12 12 12 12 12 12]\n",
      " [12 12 12 12 12 12 12 12 12 12]\n",
      " [12 12 12 12 12 12 12 12 12 12]\n",
      " [12 12 12 12 12 12 12 12 12 12]\n",
      " [ 0 12 12  0  0  0 12  0  0 12]\n",
      " [ 0  0 12  0  0  0 12  0  0 12]\n",
      " [ 0  0 12  0  0  0 12  0  0  0]\n",
      " [ 0  0 12  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]] (27, 10)\n",
      "P es  [12 12 12 12 12 12 12 12 12 12] (10,) \n",
      "\n",
      "an (reversed: na) -> [ llll ] (KO)\n",
      "english (reversed: hsilgne) -> [ lllll ] (KO)\n",
      "sentence (reversed: ecnetnes) -> [ llllllll ] (KO)\n",
      "that (reversed: taht) -> [ llll ] (KO)\n",
      "can (reversed: nac) -> [ llll ] (KO)\n",
      "be (reversed: eb) -> [ llll ] (KO)\n",
      "translated (reversed: detalsnart) -> [ lllllll ] (KO)\n",
      "to (reversed: ot) -> [ llll ] (KO)\n",
      "the (reversed: eht) -> [ llll ] (KO)\n",
      "following (reversed: gniwollof) -> [ llllll ] (KO)\n",
      "=================\n",
      "DESPUES DEL BUCLE\n",
      "=================\n",
      "decoder_inputs:  [[12 12 12 12 12 12 12 12 12 12]\n",
      " [12 12 12 12 12 12 12 12 12 12]\n",
      " [12 12 12 12 12 12 12 12 12 12]\n",
      " [12 12 12 12 12 12 12 12 12 12]\n",
      " [12 12  0  0 12  0 12 12 12  0]\n",
      " [ 0  0  0  0  0  0  0  0 12  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]] (27, 10)\n",
      "P es  [12 12 12 12 12 12 12 12 12 12] (10,) \n",
      "\n",
      "french (reversed: hcnerf) -> [ lllll ] (KO)\n",
      "one (reversed: eno) -> [ lllll ] (KO)\n",
      "le (reversed: el) -> [ llll ] (KO)\n",
      "vif (reversed: fiv) -> [ llll ] (KO)\n",
      "renard (reversed: draner) -> [ lllll ] (KO)\n",
      "brun (reversed: nurb) -> [ llll ] (KO)\n",
      "saute (reversed: etuas) -> [ lllll ] (KO)\n",
      "par (reversed: rap) -> [ lllll ] (KO)\n",
      "dessus (reversed: sussed) -> [ llllll ] (KO)\n",
      "le (reversed: el) -> [ llll ] (KO)\n",
      "=================\n",
      "DESPUES DEL BUCLE\n",
      "=================\n",
      "decoder_inputs:  [[12 12 12 12 12 12 12 12 12 12]\n",
      " [12 12 12 12 12 12 12 12 12 12]\n",
      " [12 12 12 12 12 12 12 12 12 12]\n",
      " [12 12 12 12 12 12 12 12 12 12]\n",
      " [ 0 12  0  0  0  0 12 12 12 12]\n",
      " [ 0 12  0  0  0  0  0  0  0 12]\n",
      " [ 0 12  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]] (27, 10)\n",
      "P es  [12 12 12 12 12 12 12 12 12 12] (10,) \n",
      "\n",
      "chien (reversed: neihc) -> [ llll ] (KO)\n",
      "paresseux (reversed: xuesserap) -> [ lllllll ] (KO)\n",
      "here (reversed: ereh) -> [ llll ] (KO)\n",
      "is (reversed: si) -> [ llll ] (KO)\n",
      "an (reversed: na) -> [ llll ] (KO)\n",
      "extremely (reversed: ylemertxe) -> [ llll ] (KO)\n",
      "long (reversed: gnol) -> [ lllll ] (KO)\n",
      "french (reversed: hcnerf) -> [ lllll ] (KO)\n",
      "word (reversed: drow) -> [ lllll ] (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> [ llllllllllllllllllllllllll ] (KO)\n",
      "* correct: 0/40 -> 0.0%\n",
      "\n",
      "*** evaluation! loss: 2.09067\n",
      "=================\n",
      "DESPUES DEL BUCLE\n",
      "=================\n",
      "decoder_inputs:  [[ 5 11 14 24 19 18  5 25  7  9]\n",
      " [ 8 11 15 24 13 22  8  1 15  9]\n",
      " [20 21 15 24 13 22 20 12  4  0]\n",
      " [20 21 15  0 13 22 20 12  0  0]\n",
      " [ 0 21 15  0 13 22  0  0  0  0]\n",
      " [ 0 21  2  0 13 22  0  0  0  0]\n",
      " [ 0  0  2  0  0 22  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]] (27, 10)\n",
      "P es  [ 5 11 14 24 19 18  5 25  7  9] (10,) \n",
      "\n",
      "the (reversed: eht) -> [ ehtt ] (KO)\n",
      "quick (reversed: kciuq) -> [ kkuuuu ] (KO)\n",
      "brown (reversed: nworb) -> [ noooobb ] (KO)\n",
      "fox (reversed: xof) -> [ xxx ] (KO)\n",
      "jumps (reversed: spmuj) -> [ smmmmm ] (KO)\n",
      "over (reversed: revo) -> [ rvvvvvv ] (KO)\n",
      "the (reversed: eht) -> [ ehtt ] (KO)\n",
      "lazy (reversed: yzal) -> [ yall ] (KO)\n",
      "dog (reversed: god) -> [ god ] (OK)\n",
      "is (reversed: si) -> [ ii ] (KO)\n",
      "=================\n",
      "DESPUES DEL BUCLE\n",
      "=================\n",
      "decoder_inputs:  [[14  8  5 20 14  5  5 15  5  7]\n",
      " [ 1  8  5 20  1  2  5 20  8  7]\n",
      " [ 0 19  5 20  3  0  1  0 20 14]\n",
      " [ 0 19  5 20  3  0  1  0 20 15]\n",
      " [ 0 19  5 20  3  0  1  0  0 15]\n",
      " [ 0  5  5 20  0  0  1  0  0 15]\n",
      " [ 0  5 14 20  0  0  1  0  0 15]\n",
      " [ 0  5 19  0  0  0 14  0  0 15]\n",
      " [ 0  5 19  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0 20  0  0 12]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]] (27, 10)\n",
      "P es  [14  8  5 20 14  5  5 15  5  7] (10,) \n",
      "\n",
      "an (reversed: na) -> [ na ] (OK)\n",
      "english (reversed: hsilgne) -> [ hhssseeee ] (KO)\n",
      "sentence (reversed: ecnetnes) -> [ eeeeeensss ] (KO)\n",
      "that (reversed: taht) -> [ ttttttt ] (KO)\n",
      "can (reversed: nac) -> [ naccc ] (KO)\n",
      "be (reversed: eb) -> [ eb ] (OK)\n",
      "translated (reversed: detalsnart) -> [ eeaaaaanntttt ] (KO)\n",
      "to (reversed: ot) -> [ ot ] (OK)\n",
      "the (reversed: eht) -> [ ehtt ] (KO)\n",
      "following (reversed: gniwollof) -> [ ggnoooooll ] (KO)\n",
      "=================\n",
      "DESPUES DEL BUCLE\n",
      "=================\n",
      "decoder_inputs:  [[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 8 15 12  9  4 18  5 16 19 12]\n",
      " [ 8 15  0  9 18 18  1 16 19  0]\n",
      " [14  0  0  0 14 18  1 16 19  0]\n",
      " [14  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0 14  0  1  0 19  0]\n",
      " [18  0  0  0 14  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]] (27, 10)\n",
      "P es  [ 8  5  5  6  4 14  5 18 19  5] (10,) \n",
      "\n",
      "french (reversed: hcnerf) -> [ hhhnnrr ] (KO)\n",
      "one (reversed: eno) -> [ eoo ] (KO)\n",
      "le (reversed: el) -> [ el ] (OK)\n",
      "vif (reversed: fiv) -> [ fii ] (KO)\n",
      "renard (reversed: draner) -> [ ddrnnnn ] (KO)\n",
      "brun (reversed: nurb) -> [ nrrrb ] (KO)\n",
      "saute (reversed: etuas) -> [ eeaaaa ] (KO)\n",
      "par (reversed: rap) -> [ rppp ] (KO)\n",
      "dessus (reversed: sussed) -> [ ssssss ] (KO)\n",
      "le (reversed: el) -> [ el ] (OK)\n",
      "=================\n",
      "DESPUES DEL BUCLE\n",
      "=================\n",
      "decoder_inputs:  [[14 24  5  9 14 25  7  8  4 12]\n",
      " [14 24  5  9  1  5 14  8 18 12]\n",
      " [ 8  5  5  0  0  5 15  8 15 12]\n",
      " [ 8 19  5  0  0  5 15 14 15 12]\n",
      " [ 8 19  5  0  0  5  0 14 15 12]\n",
      " [ 8 19  5  0  0  5  0 18  0 12]\n",
      " [ 8 19  5  0  0  5  0 18  0 12]\n",
      " [ 0  5  0  0  0  5  0  0  0 12]\n",
      " [ 0 16  0  0  0  5  0  0  0 12]\n",
      " [ 0 16  0  0  0  5  0  0  0 12]\n",
      " [ 0 16  0  0  0  5  0  0  0 12]\n",
      " [ 0 16  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]] (27, 10)\n",
      "P es  [14 24  5  9 14 25  7  8  4 12] (10,) \n",
      "\n",
      "chien (reversed: neihc) -> [ nnhhhhh ] (KO)\n",
      "paresseux (reversed: xuesserap) -> [ xxessssepppp ] (KO)\n",
      "here (reversed: ereh) -> [ eeeeeee ] (KO)\n",
      "is (reversed: si) -> [ ii ] (KO)\n",
      "an (reversed: na) -> [ na ] (OK)\n",
      "extremely (reversed: ylemertxe) -> [ yeeeeeeeeee ] (KO)\n",
      "long (reversed: gnol) -> [ gnoo ] (KO)\n",
      "french (reversed: hcnerf) -> [ hhhnnrr ] (KO)\n",
      "word (reversed: drow) -> [ drooo ] (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> [ lllllllllllllttttttooccaaa ] (KO)\n",
      "* correct: 7/40 -> 17.5%\n",
      "\n",
      "CPU times: user 12min 47s, sys: 22.6 s, total: 13min 10s\n",
      "Wall time: 5min 34s\n"
     ]
    }
   ],
   "source": [
    "#%time reverse_text(15000)\n",
    "tf.reset_default_graph()\n",
    "%time reverse_text(1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "%time reverse_text(30000)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
